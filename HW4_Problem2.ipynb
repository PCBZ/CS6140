{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNffTXhOthHg+7ICsHUk3oO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PCBZ/CS6140/blob/main/HW4/HW4_Problem2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czdZu5wOIK5O",
        "outputId": "1648a005-284e-422c-bf66-9619218e8364"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Searching for Best Parameters\n",
            "==================================================\n",
            "\n",
            "[1/1] Testing: {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 3}\n",
            "  Train: 0.7597, Test: 0.6584, Gap: 0.1013\n",
            "\n",
            "==================================================\n",
            "Best Model Analysis\n",
            "==================================================\n",
            "Best parameters: {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 3}\n",
            "Best test accuracy: 0.6584\n",
            "\n",
            "Per-class accuracy (best model):\n",
            "  comp.graphics: 81.23%\n",
            "  comp.os.ms-windows.misc: 58.38%\n",
            "  comp.sys.ibm.pc.hardware: 64.54%\n",
            "  comp.sys.mac.hardware: 60.78%\n",
            "  rec.autos: 62.12%\n",
            "  rec.motorcycles: 56.03%\n",
            "  rec.sport.baseball: 68.01%\n",
            "  rec.sport.hockey: 75.69%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "class GradientBoostingClassifier:\n",
        "    \"\"\"\n",
        "    Gradient Boosting Classifier using decision trees as weak learners.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3, min_samples_split=2, min_samples_leaf=1):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the model to the training data.\n",
        "        \"\"\"\n",
        "        self.classes_ = np.unique(y)\n",
        "        self.n_classes_ = len(self.classes_)\n",
        "\n",
        "        n_samples, _ = X.shape\n",
        "\n",
        "        # Initial predictions\n",
        "        class_scores = np.zeros((n_samples, self.n_classes_))\n",
        "        self.initial_predictions = class_scores.copy()\n",
        "        self.trees = []\n",
        "\n",
        "        # Create one-hot encoding using np.eye\n",
        "        y_one_hot = np.eye(self.n_classes_)[y]\n",
        "\n",
        "        for i in range(self.n_estimators):\n",
        "            # Convert scores to probabilities using softmax\n",
        "            exp_scores = np.exp(class_scores - np.max(class_scores, axis=1, keepdims=True))\n",
        "            probabilities = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "            # Calculate residuals for all classes\n",
        "            residuals = y_one_hot - probabilities\n",
        "\n",
        "            tree_list = []\n",
        "            for j in range(self.n_classes_):\n",
        "                # Fit a decision tree to the residuals\n",
        "                tree = DecisionTreeRegressor(max_depth=self.max_depth, min_samples_split=self.min_samples_split, min_samples_leaf=self.min_samples_split)\n",
        "                tree.fit(X, residuals[:, j])\n",
        "                tree_list.append(tree)\n",
        "\n",
        "                # Update the predictions\n",
        "                class_scores[:, j] += self.learning_rate * tree.predict(X)\n",
        "\n",
        "            self.trees.append(tree_list)\n",
        "\n",
        "    def _softmax(self, F):\n",
        "        \"\"\"\n",
        "        Compute softmax probabilities from raw scores.\n",
        "        \"\"\"\n",
        "        # Numerical stability: subtract max\n",
        "        F_stable = F - np.max(F, axis=1, keepdims=True)\n",
        "        exp_F = np.exp(F_stable)\n",
        "        return exp_F / np.sum(exp_F, axis=1, keepdims=True)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predict the class probabilities for the given data.\n",
        "        \"\"\"\n",
        "        n_samples, _ = X.shape\n",
        "\n",
        "        # Initialize with zeros (since we're not using initial log-odds anymore)\n",
        "        class_scores = np.zeros((n_samples, self.n_classes_))\n",
        "\n",
        "        # Accumulate predictions from all trees\n",
        "        for tree_list in self.trees:\n",
        "            for j in range(self.n_classes_):\n",
        "                class_scores[:, j] += self.learning_rate * tree_list[j].predict(X)\n",
        "\n",
        "        # Convert to probabilities using softmax\n",
        "        exp_scores = np.exp(class_scores - np.max(class_scores, axis=1, keepdims=True))\n",
        "        probabilities = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "        return probabilities\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class for given data.\n",
        "        \"\"\"\n",
        "        probabilities = self.predict_proba(X)\n",
        "        return np.argmax(probabilities, axis=1)\n",
        "\n",
        "def fetch_data():\n",
        "    \"\"\"\n",
        "    Fetch the 20 Newsgroups dataset, vectorize the text using TF-IDF,\n",
        "    and split into train and test sets.\n",
        "    \"\"\"\n",
        "    categories = [\n",
        "        'comp.graphics',\n",
        "        'comp.os.ms-windows.misc',\n",
        "        'comp.sys.ibm.pc.hardware',\n",
        "        'comp.sys.mac.hardware',\n",
        "        'rec.autos',\n",
        "        'rec.motorcycles',\n",
        "        'rec.sport.baseball',\n",
        "        'rec.sport.hockey',\n",
        "    ]\n",
        "\n",
        "    newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "    X_train_text, y_train = newsgroups_train.data, newsgroups_train.target\n",
        "\n",
        "    newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "    X_test_text, y_test = newsgroups_test.data, newsgroups_test.target\n",
        "\n",
        "    # TF-IDF vectorization\n",
        "    vectorizer = TfidfVectorizer(max_features=2000)\n",
        "\n",
        "    X_train = vectorizer.fit_transform(X_train_text)\n",
        "    X_test = vectorizer.transform(X_test_text)\n",
        "\n",
        "    return X_train, y_train, X_test, y_test, newsgroups_train.target_names\n",
        "\n",
        "\n",
        "def find_best_parameters(data):\n",
        "    \"\"\"\n",
        "    Find the best parameters for the model.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Searching for Best Parameters\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    X_train, y_train, X_test, y_test, target_names = data\n",
        "\n",
        "    param_grid = [\n",
        "        {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 3},\n",
        "    ]\n",
        "\n",
        "    results = []\n",
        "    best_test_acc = 0\n",
        "    best_params = None\n",
        "    best_model = None\n",
        "\n",
        "    # Test params\n",
        "    for idx, params in enumerate(param_grid):\n",
        "        print(f\"\\n[{idx+1}/{len(param_grid)}] Testing: {params}\")\n",
        "\n",
        "        # train model\n",
        "        model = GradientBoostingClassifier(**params)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate\n",
        "        train_pred = model.predict(X_train)\n",
        "        test_pred = model.predict(X_test)\n",
        "\n",
        "        train_acc = accuracy_score(y_train, train_pred)\n",
        "        test_acc = accuracy_score(y_test, test_pred)\n",
        "\n",
        "\n",
        "        result = {\n",
        "            'params': params,\n",
        "            'train_acc': train_acc,\n",
        "            'test_acc': test_acc,\n",
        "            'gap': train_acc - test_acc,\n",
        "            'model': model\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "        print(f\"  Train: {train_acc:.4f}, Test: {test_acc:.4f}, Gap: {train_acc - test_acc:.4f}\")\n",
        "\n",
        "        # Update\n",
        "        if test_acc > best_test_acc:\n",
        "            best_test_acc = test_acc\n",
        "            best_params = params\n",
        "            best_model = model\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Best Model Analysis\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Best parameters: {best_params}\")\n",
        "    print(f\"Best test accuracy: {best_test_acc:.4f}\")\n",
        "\n",
        "\n",
        "    test_pred = best_model.predict(X_test)\n",
        "    print(\"\\nPer-class accuracy (best model):\")\n",
        "    for i in range(best_model.n_classes_):\n",
        "        mask = y_test == i\n",
        "        if np.sum(mask) > 0:\n",
        "            class_acc = accuracy_score(y_test[mask], test_pred[mask])\n",
        "            class_name = target_names[i]\n",
        "            print(f\"  {class_name}: {class_acc:.2%}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Load complete dataset\n",
        "    data = fetch_data()\n",
        "\n",
        "    find_best_parameters(data)"
      ]
    }
  ]
}