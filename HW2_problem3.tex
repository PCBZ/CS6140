\documentclass[12pt]{article}
\usepackage[margin=0.7in]{geometry}
\usepackage{parskip}
\usepackage{amsmath}

\title{Problem 3}
\author{}
\date{}

\begin{document}
% \maketitle
\thispagestyle{empty}

\section*{Andrew Ng's Lecture on Applying Machine Learning}

\subsection*{Core Diagnostic Framework: Bias vs. Variance}

\begin{itemize}
\item \textbf{High Variance (Overfitting)}: Training error $\ll$ test error
   \begin{itemize}
   \item Fixes: More training data, fewer features, increase $\lambda$
   \end{itemize}
\item \textbf{High Bias (Underfitting)}: Both training and test error high
   \begin{itemize}
   \item Fixes: More features, polynomial features, decrease $\lambda$
   \end{itemize}
\item \textbf{Diagnostic}: Plot learning curves (error vs. training set size $m$)
\end{itemize}

\subsection*{Additional Diagnostics}

\begin{itemize}
\item \textbf{Optimization vs. Objective Function}:
   \begin{itemize}
   \item If $J(\theta_{SVM}) > J(\theta_{LR})$, $a(\theta_{SVM}) > a(\theta_{LR})$ $\rightarrow$ optimization problem
   \item If $J(\theta_{SVM}) \leq J(\theta_{LR})$, $a(\theta_{SVM}) > a(\theta_{LR})$ $\rightarrow$ wrong objective
   \end{itemize}
\item \textbf{Error Analysis}: Replace pipeline components with ground truth, measure impact
\item \textbf{Ablative Analysis}: Remove features/components, measure degradation
\end{itemize}

\subsection*{Getting Started: 2 Approaches}

\begin{enumerate}
\item \textbf{Careful Design}: Perfect features upfront $\rightarrow$ Risk: premature optimization
\item \textbf{Build-and-Fix} (Recommended): Quick implementation $\rightarrow$ diagnostics $\rightarrow$ targeted fixes
\end{enumerate}

\subsection*{Practical Workflow}

\begin{enumerate}
\item Implement baseline quickly
\item Run diagnostics to identify problem type
\item Apply appropriate fixes:
   \begin{itemize}
   \item Variance problem $\rightarrow$ regularization/data/features
   \item Bias problem $\rightarrow$ complexity/features
   \item Optimization problem $\rightarrow$ algorithm/iterations
   \end{itemize}
\item Iterate based on diagnostic results
\end{enumerate}

\subsection*{Key Principles}

\begin{itemize}
\item \textbf{Diagnostics $>$ guessing}: Time spent on diagnostics is time well spent
\item \textbf{Start simple}: Plot data first, implement baseline, then improve
\item \textbf{Avoid premature optimization}: You can't know what needs work until you try
\item \textbf{Let data guide you}: Error analysis reveals where to focus effort
\end{itemize}

\newpage
\thispagestyle{empty}

\section*{A Few Useful Things to Know About Machine learning}

\subsection*{Core Ideas}
\begin{itemize}
\item Learning = Representation + Evaluation + Optimization
\item Goal: Generalize to new data, not memorize training data
\item Data alone isn't enough -- algorithms need built-in assumptions
\item Overfitting is the main enemy
\end{itemize}

\subsection*{Practical Tips}
\begin{itemize}
\item More data $>$ fancier algorithms
\item Feature engineering matters most
\item Combine multiple models for better results
\item Start with simple algorithms first
\end{itemize}

\subsection*{Common Mistakes}
\begin{itemize}
\item High dimensions break intuition
\item Theoretical guarantees are too pessimistic
\item Simple $\neq$ accurate
\item Can represent $\neq$ can learn
\end{itemize}

\subsection*{What Works}
\begin{itemize}
\item Use diagnostics to fix problems
\item Cross-validate carefully
\item Remember: correlation $\neq$ causation
\item Optimize for human time, not just accuracy
\end{itemize}

\end{document}