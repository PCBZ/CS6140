{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEg+//ht/0Vl2QzceT++H5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PCBZ/CS6140/blob/main/HW4/HW4_Problem2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czdZu5wOIK5O",
        "outputId": "be44cece-cb1f-494a-d3e0-f459b029386e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 8newsgroup dataset...\n",
            "Removing existing data directory: ./data\n",
            "Downloading dataset from https://www.khoury.northeastern.edu/home/vip/teach/MLcourse/data/8newsgroup.zip...\n",
            "Download completed!\n",
            "Extracting dataset...\n",
            "Extraction completed!\n",
            "Removed zip file.\n",
            "\n",
            "Using directories:\n",
            "  Train: ./data/train.trec\n",
            "  Test: ./data/test.trec\n",
            "\n",
            "Loaded config: {'numClasses': 8, 'numDataPoints': 11314, 'numFeatures': 1754}\n",
            "\n",
            "Loading training data...\n",
            "Loading test data...\n",
            "\n",
            "Dataset loaded successfully!\n",
            "  Training: 11314 samples\n",
            "  Test: 7532 samples\n",
            "  Features: 1754\n",
            "  Classes: 8\n",
            "  Sparsity: 98.9%\n",
            "\n",
            "Dataset summary:\n",
            "  Training samples: 11314\n",
            "  Test samples: 7532\n",
            "  Feature dimension: 1754\n",
            "  Number of classes: 8\n",
            "  Classes: [0 1 2 3 4 5 6 7]\n",
            "\n",
            "==================================================\n",
            "Searching for Best Parameters\n",
            "==================================================\n",
            "\n",
            "[1/1] Testing: {'n_estimators': 150, 'learning_rate': 0.1, 'max_depth': 10, 'min_sample_split': 2}\n",
            "  Train: 0.9547, Test: 0.8456, Gap: 0.1092\n",
            "  Training time: 232.4s\n",
            "\n",
            "==================================================\n",
            "Results Summary\n",
            "==================================================\n",
            "\n",
            "Top 5 configurations by test accuracy:\n",
            "\n",
            "1. {'n_estimators': 150, 'learning_rate': 0.1, 'max_depth': 10, 'min_sample_split': 2}\n",
            "   Test: 0.8456, Train: 0.9547, Gap: 0.1092\n",
            "\n",
            "==================================================\n",
            "Best Model Analysis\n",
            "==================================================\n",
            "Best parameters: {'n_estimators': 150, 'learning_rate': 0.1, 'max_depth': 10, 'min_sample_split': 2}\n",
            "Best test accuracy: 0.8456\n",
            "\n",
            "Per-class accuracy (best model):\n",
            "  religion: 85.12%\n",
            "  graphics: 95.48%\n",
            "  windows: 74.36%\n",
            "  ibm.hardware: 78.97%\n",
            "  mac.hardware: 87.81%\n",
            "  windows.x: 54.29%\n",
            "  forsale: 73.86%\n",
            "  autos: 76.48%\n",
            "\n",
            "==================================================\n",
            "Best Model Summary\n",
            "==================================================\n",
            "Best parameters: {'n_estimators': 150, 'learning_rate': 0.1, 'max_depth': 10, 'min_sample_split': 2}\n"
          ]
        }
      ],
      "source": [
        "from re import X\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "import shutil\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "import time\n",
        "\n",
        "\n",
        "class GradientBoostingClassifier:\n",
        "    \"\"\"\n",
        "    Gradient Boosting Classifier using decision trees as weak learners.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3, min_sample_split=2):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.min_sample_split = min_sample_split\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the model to the training data.\n",
        "        \"\"\"\n",
        "        self.classes_ = np.unique(y)\n",
        "        self.n_classes_ = len(self.classes_)\n",
        "\n",
        "        n_samples, _ = X.shape\n",
        "\n",
        "        # Initialize the target variable\n",
        "        class_scores = np.zeros((n_samples, self.n_classes_))\n",
        "\n",
        "        # Calculate initial log-odds\n",
        "        self.initial_predictions = np.zeros(self.n_classes_)\n",
        "        for i in range(self.n_classes_):\n",
        "            p = np.mean(y == i)\n",
        "            self.initial_predictions[i] = np.log((p + 1e-8) / (1 - p + 1e-8))\n",
        "            class_scores[:, i] = self.initial_predictions[i]\n",
        "\n",
        "        self.trees = [[] for _ in range(self.n_classes_)]\n",
        "\n",
        "        for i in range(self.n_estimators):\n",
        "            # For each class, fit a tree\n",
        "            for j in range(self.n_classes_):\n",
        "                # Binary Labels\n",
        "                y_bin = (y == j).astype(int)\n",
        "\n",
        "                # Calculate the predict probability using sigmoid\n",
        "                y_pred_prob = 1 / (1 + np.exp(-class_scores[:, j]))\n",
        "\n",
        "                # Calculate the residuals\n",
        "                residuals = y_bin - y_pred_prob\n",
        "\n",
        "                # Fit a decision tree to the residuals\n",
        "                tree = DecisionTreeRegressor(max_depth=self.max_depth, min_samples_split=self.min_sample_split)\n",
        "                tree.fit(X, residuals)\n",
        "\n",
        "                # Update the predictions\n",
        "                class_scores[:, j] += self.learning_rate * tree.predict(X)\n",
        "\n",
        "                self.trees[j].append(tree)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predict the class probabilities for the given data.\n",
        "        \"\"\"\n",
        "        n_samples, _ = X.shape\n",
        "\n",
        "        # Initialize with initial predictions\n",
        "        class_scores = np.zeros((n_samples, self.n_classes_))\n",
        "        for i in range(self.n_classes_):\n",
        "            class_scores[:, i] = self.initial_predictions[i]\n",
        "\n",
        "        # Accumulate predictions\n",
        "        for i in range(self.n_classes_):\n",
        "            for tree in self.trees[i]:\n",
        "                class_scores[:, i] += self.learning_rate * tree.predict(X)\n",
        "\n",
        "        # Convert to probabilities\n",
        "        probabilities = 1 / (1 + np.exp(-class_scores))\n",
        "\n",
        "        # Normalize\n",
        "        probabilities /= np.sum(probabilities, axis=1, keepdims=True)\n",
        "\n",
        "        return probabilities\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class for given data.\n",
        "        \"\"\"\n",
        "        probabilities = self.predict_proba(X)\n",
        "        return np.argmax(probabilities, axis=1)\n",
        "\n",
        "\n",
        "def download_and_extract_data(url='https://www.khoury.northeastern.edu/home/vip/teach/MLcourse/data/8newsgroup.zip',\n",
        "                              data_dir='./data'):\n",
        "    \"\"\"\n",
        "    Download and extract 8newsgroup dataset. Always downloads fresh copy.\n",
        "    \"\"\"\n",
        "    # Remove existing data directory to ensure fresh download\n",
        "    if os.path.exists(data_dir):\n",
        "        print(f\"Removing existing data directory: {data_dir}\")\n",
        "        shutil.rmtree(data_dir)\n",
        "\n",
        "    # Create data directory\n",
        "    os.makedirs(data_dir)\n",
        "\n",
        "    zip_path = os.path.join(data_dir, '8newsgroup.zip')\n",
        "\n",
        "    # Download dataset\n",
        "    print(f\"Downloading dataset from {url}...\")\n",
        "    try:\n",
        "        urllib.request.urlretrieve(url, zip_path)\n",
        "        print(\"Download completed!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading file: {e}\")\n",
        "        raise\n",
        "\n",
        "    # Extract dataset\n",
        "    print(\"Extracting dataset...\")\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(data_dir)\n",
        "        print(\"Extraction completed!\")\n",
        "\n",
        "        # Remove zip file to save space\n",
        "        os.remove(zip_path)\n",
        "        print(\"Removed zip file.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting file: {e}\")\n",
        "        raise\n",
        "\n",
        "    return data_dir\n",
        "\n",
        "\n",
        "def load_sparse_data(file_path):\n",
        "    \"\"\"\n",
        "    Load sparse matrix data from file.\n",
        "    Format: label feature_id:value feature_id:value ...\n",
        "    \"\"\"\n",
        "    labels = []\n",
        "    rows = []\n",
        "    cols = []\n",
        "    values = []\n",
        "\n",
        "    with open(file_path, 'r') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) > 0:  # Skip empty lines\n",
        "                labels.append(int(parts[0]))\n",
        "\n",
        "                for feat_val in parts[1:]:\n",
        "                    if ':' in feat_val:  # Make sure it's a valid feature:value pair\n",
        "                        feat_id, val = feat_val.split(':')\n",
        "                        rows.append(i)\n",
        "                        cols.append(int(feat_id))\n",
        "                        values.append(float(val))\n",
        "\n",
        "    return labels, (values, (rows, cols))\n",
        "\n",
        "\n",
        "def load_and_prepare_data():\n",
        "    \"\"\"\n",
        "    Load 8newsgroup dataset.\n",
        "    \"\"\"\n",
        "    print(\"Loading 8newsgroup dataset...\")\n",
        "\n",
        "    # Download and extract (always fresh)\n",
        "    data_path = download_and_extract_data()\n",
        "\n",
        "    # Based on the typical structure, the files should be in:\n",
        "    # data/train.trec/ and data/test.trec/\n",
        "    train_dir = os.path.join(data_path, 'train.trec')\n",
        "    test_dir = os.path.join(data_path, 'test.trec')\n",
        "\n",
        "    # Check if directories exist\n",
        "    if not os.path.exists(train_dir):\n",
        "        print(f\"\\nTrain directory not found at: {train_dir}\")\n",
        "        print(\"Checking alternative locations...\")\n",
        "\n",
        "        # Try to find the correct structure\n",
        "        for root, dirs, files in os.walk(data_path):\n",
        "            if 'train.trec' in dirs:\n",
        "                train_dir = os.path.join(root, 'train.trec')\n",
        "                test_dir = os.path.join(root, 'test.trec')\n",
        "                print(f\"Found directories at: {root}\")\n",
        "                break\n",
        "\n",
        "    if not os.path.exists(train_dir):\n",
        "        raise ValueError(f\"Could not find training directory. Expected at: {train_dir}\")\n",
        "\n",
        "    print(f\"\\nUsing directories:\")\n",
        "    print(f\"  Train: {train_dir}\")\n",
        "    print(f\"  Test: {test_dir}\")\n",
        "\n",
        "    # Load config\n",
        "    config = {}\n",
        "    config_file = os.path.join(train_dir, 'config.txt')\n",
        "    if os.path.exists(config_file):\n",
        "        with open(config_file, 'r') as f:\n",
        "            for line in f:\n",
        "                if '=' in line and not line.startswith('#'):\n",
        "                    key, val = line.strip().split('=')\n",
        "                    config[key] = int(val)\n",
        "        n_features = config.get('numFeatures', 1754)\n",
        "        print(f\"\\nLoaded config: {config}\")\n",
        "    else:\n",
        "        print(f\"Warning: config.txt not found at {config_file}\")\n",
        "        n_features = 1754\n",
        "\n",
        "    # Load training data\n",
        "    print(\"\\nLoading training data...\")\n",
        "    train_file = os.path.join(train_dir, 'feature_matrix.txt')\n",
        "    if not os.path.exists(train_file):\n",
        "        raise ValueError(f\"Training data file not found: {train_file}\")\n",
        "\n",
        "    y_train, train_data = load_sparse_data(train_file)\n",
        "    X_train = csr_matrix(train_data, shape=(len(y_train), n_features))\n",
        "    y_train = np.array(y_train)\n",
        "\n",
        "    # Load test data\n",
        "    print(\"Loading test data...\")\n",
        "    test_file = os.path.join(test_dir, 'feature_matrix.txt')\n",
        "    y_test, test_data = load_sparse_data(test_file)\n",
        "    X_test = csr_matrix(test_data, shape=(len(y_test), n_features))\n",
        "    y_test = np.array(y_test)\n",
        "\n",
        "    # Get category names\n",
        "    categories = ['religion', 'graphics', 'windows', 'ibm.hardware',\n",
        "                  'mac.hardware', 'windows.x', 'forsale', 'autos']\n",
        "\n",
        "    print(f\"\\nDataset loaded successfully!\")\n",
        "    print(f\"  Training: {X_train.shape[0]} samples\")\n",
        "    print(f\"  Test: {X_test.shape[0]} samples\")\n",
        "    print(f\"  Features: {n_features}\")\n",
        "    print(f\"  Classes: {len(categories)}\")\n",
        "    print(f\"  Sparsity: {1 - X_train.nnz / (X_train.shape[0] * n_features):.1%}\")\n",
        "\n",
        "    return {\n",
        "        'X_train': X_train,\n",
        "        'y_train': y_train,\n",
        "        'X_test': X_test,\n",
        "        'y_test': y_test,\n",
        "        'vectorizer': None,\n",
        "        'target_names': categories\n",
        "    }\n",
        "\n",
        "def run_training_and_evaluate(data):\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Starting Complete Training Process\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    X_train, y_train = data['X_train'], data['y_train']\n",
        "    X_test, y_test = data['X_test'], data['y_test']\n",
        "\n",
        "    # Create model and train\n",
        "    model = GradientBoostingClassifier(n_estimators=200, learning_rate=0.05, max_depth=5)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    train_accuracy = accuracy_score(y_train, model.predict(X_train))\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Training accuracy: {train_accuracy:.2%}\")\n",
        "    print(f\"Test accuracy: {test_accuracy:.2%}\")\n",
        "\n",
        "    # Per-class accuracy\n",
        "    print(\"\\nPer-class accuracy (Test set):\")\n",
        "    for i in range(model.n_classes_):\n",
        "        mask = data['y_test'] == i\n",
        "        if np.sum(mask) > 0:\n",
        "            class_acc = accuracy_score(data['y_test'][mask], y_pred[mask])\n",
        "            class_name = data['target_names'][i] if i < len(data['target_names']) else f\"Class {i}\"\n",
        "            print(f\"  {class_name}: {class_acc:.2%}\")\n",
        "\n",
        "    # # Confusion matrix\n",
        "    # cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # # Create figure\n",
        "    # plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # # Plot heatmap\n",
        "    # sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "    #             xticklabels=data['target_names'],\n",
        "    #             yticklabels=data['target_names'])\n",
        "\n",
        "    # plt.title('Confusion Matrix - Gradient Boosting Classifier')\n",
        "    # plt.xlabel('Predicted Label')\n",
        "    # plt.ylabel('True Label')\n",
        "    # plt.tight_layout()\n",
        "\n",
        "    # plt.show()\n",
        "\n",
        "def find_best_parameters(data):\n",
        "    \"\"\"\n",
        "    Find the best parameters for the model.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Searching for Best Parameters\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Define parameters combine\n",
        "    param_grid = [\n",
        "        {'n_estimators': 150, 'learning_rate': 0.1, 'max_depth': 10, 'min_sample_split': 2},\n",
        "        # {'n_estimators': 150, 'learning_rate': 0.1, 'max_depth': 8, 'min_sample_split': 20}\n",
        "    ]\n",
        "\n",
        "    results = []\n",
        "    best_test_acc = 0\n",
        "    best_params = None\n",
        "    best_model = None\n",
        "\n",
        "    # Test params\n",
        "    for idx, params in enumerate(param_grid):\n",
        "        print(f\"\\n[{idx+1}/{len(param_grid)}] Testing: {params}\")\n",
        "\n",
        "        # train model\n",
        "        model = GradientBoostingClassifier(**params)\n",
        "        start_time = time.time()\n",
        "        model.fit(data['X_train'], data['y_train'])\n",
        "        train_time = time.time() - start_time\n",
        "\n",
        "        # Evaluate\n",
        "        train_pred = model.predict(data['X_train'])\n",
        "        test_pred = model.predict(data['X_test'])\n",
        "\n",
        "        train_acc = accuracy_score(data['y_train'], train_pred)\n",
        "        test_acc = accuracy_score(data['y_test'], test_pred)\n",
        "\n",
        "\n",
        "        result = {\n",
        "            'params': params,\n",
        "            'train_acc': train_acc,\n",
        "            'test_acc': test_acc,\n",
        "            'gap': train_acc - test_acc,\n",
        "            'train_time': train_time,\n",
        "            'model': model\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "        print(f\"  Train: {train_acc:.4f}, Test: {test_acc:.4f}, Gap: {train_acc - test_acc:.4f}\")\n",
        "        print(f\"  Training time: {train_time:.1f}s\")\n",
        "\n",
        "        # Update\n",
        "        if test_acc > best_test_acc:\n",
        "            best_test_acc = test_acc\n",
        "            best_params = params\n",
        "            best_model = model\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Results Summary\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Sort top 5 accuracy\n",
        "    results_sorted = sorted(results, key=lambda x: x['test_acc'], reverse=True)\n",
        "\n",
        "    print(\"\\nTop 5 configurations by test accuracy:\")\n",
        "    for i, r in enumerate(results_sorted[:5]):\n",
        "        print(f\"\\n{i+1}. {r['params']}\")\n",
        "        print(f\"   Test: {r['test_acc']:.4f}, Train: {r['train_acc']:.4f}, Gap: {r['gap']:.4f}\")\n",
        "\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Best Model Analysis\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Best parameters: {best_params}\")\n",
        "    print(f\"Best test accuracy: {best_test_acc:.4f}\")\n",
        "\n",
        "\n",
        "    test_pred = best_model.predict(data['X_test'])\n",
        "    print(\"\\nPer-class accuracy (best model):\")\n",
        "    for i in range(best_model.n_classes_):\n",
        "        mask = data['y_test'] == i\n",
        "        if np.sum(mask) > 0:\n",
        "            class_acc = accuracy_score(data['y_test'][mask], test_pred[mask])\n",
        "            class_name = data['target_names'][i]\n",
        "            print(f\"  {class_name}: {class_acc:.2%}\")\n",
        "\n",
        "    return best_model, best_params, results\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Load complete dataset\n",
        "    data = load_and_prepare_data()\n",
        "\n",
        "    print(f\"\\nDataset summary:\")\n",
        "    print(f\"  Training samples: {data['X_train'].shape[0]}\")\n",
        "    print(f\"  Test samples: {data['X_test'].shape[0]}\")\n",
        "    print(f\"  Feature dimension: {data['X_train'].shape[1]}\")\n",
        "    print(f\"  Number of classes: {len(np.unique(data['y_train']))}\")\n",
        "    print(f\"  Classes: {np.unique(data['y_train'])}\")\n",
        "\n",
        "    # run_training_and_evaluate(data)\n",
        "    best_model, best_params, results = find_best_parameters(data)\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Best Model Summary\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Best parameters: {best_params}\")"
      ]
    }
  ]
}