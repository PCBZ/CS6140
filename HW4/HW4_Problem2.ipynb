{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKmm6gzqNse3pmOpXaPPhv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PCBZ/CS6140/blob/main/HW4/HW4_Problem2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czdZu5wOIK5O",
        "outputId": "4daa3332-be9d-4b87-9cb6-1fbf09bc3ed3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Searching for Best Parameters\n",
            "==================================================\n",
            "\n",
            "[1/2] Testing: {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 5, 'min_sample_split': 20}\n",
            "  Train: 0.8574, Test: 0.7939, Gap: 0.0635\n",
            "\n",
            "[2/2] Testing: {'n_estimators': 150, 'learning_rate': 0.1, 'max_depth': 6, 'min_sample_split': 5}\n",
            "  Train: 0.9073, Test: 0.8255, Gap: 0.0817\n",
            "\n",
            "==================================================\n",
            "Best Model Analysis\n",
            "==================================================\n",
            "Best parameters: {'n_estimators': 150, 'learning_rate': 0.1, 'max_depth': 6, 'min_sample_split': 5}\n",
            "Best test accuracy: 0.8255\n",
            "\n",
            "Per-class accuracy (best model):\n",
            "  religion: 80.99%\n",
            "  computer: 95.30%\n",
            "  forsale: 72.31%\n",
            "  autos: 76.57%\n",
            "  sports: 85.68%\n",
            "  med: 48.99%\n",
            "  space: 69.54%\n",
            "  politics: 74.19%\n"
          ]
        }
      ],
      "source": [
        "from re import X\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "import shutil\n",
        "\n",
        "\n",
        "class GradientBoostingClassifier:\n",
        "    \"\"\"\n",
        "    Gradient Boosting Classifier using decision trees as weak learners.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3, min_sample_split=2):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.min_sample_split = min_sample_split\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the model to the training data.\n",
        "        \"\"\"\n",
        "        self.classes_ = np.unique(y)\n",
        "        self.n_classes_ = len(self.classes_)\n",
        "\n",
        "        n_samples, _ = X.shape\n",
        "\n",
        "        # Initialize the target variable\n",
        "        class_scores = np.zeros((n_samples, self.n_classes_))\n",
        "\n",
        "        # Calculate initial log-odds\n",
        "        self.initial_predictions = np.zeros(self.n_classes_)\n",
        "        for i in range(self.n_classes_):\n",
        "            p = np.mean(y == i)\n",
        "            self.initial_predictions[i] = np.log((p + 1e-8) / (1 - p + 1e-8))\n",
        "            class_scores[:, i] = self.initial_predictions[i]\n",
        "\n",
        "        self.trees = [[] for _ in range(self.n_classes_)]\n",
        "\n",
        "        for i in range(self.n_estimators):\n",
        "            # For each class, fit a tree\n",
        "            for j in range(self.n_classes_):\n",
        "                # Binary Labels\n",
        "                y_bin = (y == j).astype(int)\n",
        "\n",
        "                # Calculate the predict probability using sigmoid\n",
        "                y_pred_prob = 1 / (1 + np.exp(-class_scores[:, j]))\n",
        "\n",
        "                # Calculate the residuals\n",
        "                residuals = y_bin - y_pred_prob\n",
        "\n",
        "                # Fit a decision tree to the residuals\n",
        "                tree = DecisionTreeRegressor(max_depth=self.max_depth, min_samples_split=self.min_sample_split)\n",
        "                tree.fit(X, residuals)\n",
        "\n",
        "                # Update the predictions\n",
        "                class_scores[:, j] += self.learning_rate * tree.predict(X)\n",
        "\n",
        "                self.trees[j].append(tree)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predict the class probabilities for the given data.\n",
        "        \"\"\"\n",
        "        n_samples, _ = X.shape\n",
        "\n",
        "        # Initialize with initial predictions\n",
        "        class_scores = np.zeros((n_samples, self.n_classes_))\n",
        "        for i in range(self.n_classes_):\n",
        "            class_scores[:, i] = self.initial_predictions[i]\n",
        "\n",
        "        # Accumulate predictions\n",
        "        for i in range(self.n_classes_):\n",
        "            for tree in self.trees[i]:\n",
        "                class_scores[:, i] += self.learning_rate * tree.predict(X)\n",
        "\n",
        "        # Convert to probabilities\n",
        "        probabilities = 1 / (1 + np.exp(-class_scores))\n",
        "\n",
        "        # Normalize\n",
        "        probabilities /= np.sum(probabilities, axis=1, keepdims=True)\n",
        "\n",
        "        return probabilities\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class for given data.\n",
        "        \"\"\"\n",
        "        probabilities = self.predict_proba(X)\n",
        "        return np.argmax(probabilities, axis=1)\n",
        "\n",
        "def load_8newsgroup_data():\n",
        "    \"\"\"\n",
        "    Simplified 8newsgroup dataset loader\n",
        "    \"\"\"\n",
        "    import urllib.request\n",
        "    import zipfile\n",
        "    import tempfile\n",
        "    import re\n",
        "\n",
        "    # Download and extract to temporary directory\n",
        "    url = 'https://www.khoury.northeastern.edu/home/vip/teach/MLcourse/data/8newsgroup.zip'\n",
        "\n",
        "    with tempfile.TemporaryDirectory() as temp_dir:\n",
        "        # Download\n",
        "        zip_path = os.path.join(temp_dir, '8newsgroup.zip')\n",
        "        urllib.request.urlretrieve(url, zip_path)\n",
        "\n",
        "        # Extract\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(temp_dir)\n",
        "\n",
        "        # Try to read category mappings from train.trec/data_settings.txt\n",
        "        categories = {}\n",
        "        data_settings_files = []\n",
        "\n",
        "        # Look for data_settings.txt in train.trec and test.trec directories\n",
        "        for root, dirs, files in os.walk(temp_dir):\n",
        "            if 'data_settings.txt' in files and ('train.trec' in root or 'test.trec' in root):\n",
        "                data_settings_files.append(os.path.join(root, 'data_settings.txt'))\n",
        "\n",
        "        if data_settings_files:\n",
        "            # Use the first found file (preferably from train.trec)\n",
        "            setting_file = data_settings_files[0]\n",
        "\n",
        "            with open(setting_file, 'r') as f:\n",
        "                for line in f:\n",
        "                    # Parse format: intId=8496,extId=8496,intLabel=6,extLabel=space\n",
        "                    match = re.search(r'intLabel=(\\d+),extLabel=(\\w+)', line)\n",
        "                    if match:\n",
        "                        label_id = int(match.group(1))\n",
        "                        label_name = match.group(2)\n",
        "                        categories[label_id] = label_name\n",
        "\n",
        "\n",
        "        category_names = [categories[i] for i in sorted(categories.keys())]\n",
        "\n",
        "        # Load data function\n",
        "        def load_file(file_path):\n",
        "            \"\"\"Load sparse data in TREC format\"\"\"\n",
        "            labels, rows, cols, values = [], [], [], []\n",
        "\n",
        "            with open(file_path, 'r') as f:\n",
        "                for i, line in enumerate(f):\n",
        "                    parts = line.strip().split()\n",
        "                    if parts:\n",
        "                        labels.append(int(parts[0]))\n",
        "                        for feat in parts[1:]:\n",
        "                            if ':' in feat:\n",
        "                                idx, val = feat.split(':')\n",
        "                                rows.append(i)\n",
        "                                cols.append(int(idx))\n",
        "                                values.append(float(val))\n",
        "\n",
        "            return labels, (values, (rows, cols))\n",
        "\n",
        "        # Find data files\n",
        "        train_file = test_file = None\n",
        "        for root, dirs, files in os.walk(temp_dir):\n",
        "            if 'feature_matrix.txt' in files:\n",
        "                if 'train' in root:\n",
        "                    train_file = os.path.join(root, 'feature_matrix.txt')\n",
        "                elif 'test' in root:\n",
        "                    test_file = os.path.join(root, 'feature_matrix.txt')\n",
        "\n",
        "        # Load train and test data\n",
        "        y_train, train_data = load_file(train_file)\n",
        "        y_test, test_data = load_file(test_file)\n",
        "\n",
        "        # Determine number of features from the data\n",
        "        # Find the maximum feature index in both train and test data\n",
        "        train_max_feature = max(train_data[1][1]) if train_data[1][1] else 0\n",
        "        test_max_feature = max(test_data[1][1]) if test_data[1][1] else 0\n",
        "        n_features = max(train_max_feature, test_max_feature) + 1  # +1 because indices start at 0\n",
        "\n",
        "        # Convert to sparse matrices\n",
        "        X_train = csr_matrix(train_data, shape=(len(y_train), n_features))\n",
        "        X_test = csr_matrix(test_data, shape=(len(y_test), n_features))\n",
        "\n",
        "        return X_train, np.array(y_train), X_test, np.array(y_test), category_names\n",
        "\n",
        "def find_best_parameters(data):\n",
        "    \"\"\"\n",
        "    Find the best parameters for the model.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Searching for Best Parameters\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    X_train, y_train, X_test, y_test, target_names = data\n",
        "\n",
        "    # Define parameters combine\n",
        "    param_grid = [\n",
        "        # {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 2, 'min_sample_split': 2},\n",
        "        # {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 4, 'min_sample_split': 2},\n",
        "        # {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 7, 'min_sample_split': 2},\n",
        "        # {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 7, 'min_sample_split': 20},\n",
        "        # {'n_estimators': 150, 'learning_rate': 0.1, 'max_depth': 2, 'min_sample_split': 2},\n",
        "        # {'n_estimators': 150, 'learning_rate': 0.1, 'max_depth': 5, 'min_sample_split': 2},\n",
        "        {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 5, 'min_sample_split': 20},\n",
        "        {'n_estimators': 150, 'learning_rate': 0.1, 'max_depth': 6, 'min_sample_split': 5},\n",
        "    ]\n",
        "\n",
        "    results = []\n",
        "    best_test_acc = 0\n",
        "    best_params = None\n",
        "    best_model = None\n",
        "\n",
        "    # Test params\n",
        "    for idx, params in enumerate(param_grid):\n",
        "        print(f\"\\n[{idx+1}/{len(param_grid)}] Testing: {params}\")\n",
        "\n",
        "        # train model\n",
        "        model = GradientBoostingClassifier(**params)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate\n",
        "        train_pred = model.predict(X_train)\n",
        "        test_pred = model.predict(X_test)\n",
        "\n",
        "        train_acc = accuracy_score(y_train, train_pred)\n",
        "        test_acc = accuracy_score(y_test, test_pred)\n",
        "\n",
        "\n",
        "        result = {\n",
        "            'params': params,\n",
        "            'train_acc': train_acc,\n",
        "            'test_acc': test_acc,\n",
        "            'gap': train_acc - test_acc,\n",
        "            'model': model\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "        print(f\"  Train: {train_acc:.4f}, Test: {test_acc:.4f}, Gap: {train_acc - test_acc:.4f}\")\n",
        "\n",
        "        # Update\n",
        "        if test_acc > best_test_acc:\n",
        "            best_test_acc = test_acc\n",
        "            best_params = params\n",
        "            best_model = model\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Best Model Analysis\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Best parameters: {best_params}\")\n",
        "    print(f\"Best test accuracy: {best_test_acc:.4f}\")\n",
        "\n",
        "\n",
        "    test_pred = best_model.predict(X_test)\n",
        "    print(\"\\nPer-class accuracy (best model):\")\n",
        "    for i in range(best_model.n_classes_):\n",
        "        mask = y_test == i\n",
        "        if np.sum(mask) > 0:\n",
        "            class_acc = accuracy_score(y_test[mask], test_pred[mask])\n",
        "            class_name = target_names[i]\n",
        "            print(f\"  {class_name}: {class_acc:.2%}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Load complete dataset\n",
        "    data = load_8newsgroup_data()\n",
        "\n",
        "    find_best_parameters(data)"
      ]
    }
  ]
}