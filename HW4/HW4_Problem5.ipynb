{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1qhX6ot79msOjFR+6LzjC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PCBZ/CS6140/blob/main/HW4/HW4_Problem5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "class GradientBoostingClassifier:\n",
        "    \"\"\"\n",
        "    Gradient Boosting Classifier using decision trees as weak learners.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=2):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the model to the training data.\n",
        "        \"\"\"\n",
        "        self.classes_ = np.unique(y)\n",
        "        self.n_classes_ = len(self.classes_)\n",
        "\n",
        "        n_samples, _ = X.shape\n",
        "\n",
        "        # Initial predictions\n",
        "        class_scores = np.zeros((n_samples, self.n_classes_))\n",
        "        self.initial_predictions = class_scores.copy()\n",
        "        self.trees = []\n",
        "\n",
        "        # Create one-hot encoding using np.eye\n",
        "        y_one_hot = np.eye(self.n_classes_)[y]\n",
        "\n",
        "        for i in range(self.n_estimators):\n",
        "            # Convert scores to probabilities using softmax\n",
        "            exp_scores = np.exp(class_scores - np.max(class_scores, axis=1, keepdims=True))\n",
        "            probabilities = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "            # Calculate residuals for all classes\n",
        "            residuals = y_one_hot - probabilities\n",
        "\n",
        "            tree_list = []\n",
        "            for j in range(self.n_classes_):\n",
        "                # Fit a decision tree to the residuals\n",
        "                tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "                tree.fit(X, residuals[:, j])\n",
        "                tree_list.append(tree)\n",
        "\n",
        "                # Update the predictions\n",
        "                class_scores[:, j] += self.learning_rate * tree.predict(X)\n",
        "\n",
        "            self.trees.append(tree_list)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predict the class probabilities for the given data.\n",
        "        \"\"\"\n",
        "        n_samples, _ = X.shape\n",
        "\n",
        "        # Initialize with zeros (since we're not using initial log-odds anymore)\n",
        "        class_scores = np.zeros((n_samples, self.n_classes_))\n",
        "\n",
        "        # Accumulate predictions from all trees\n",
        "        for tree_list in self.trees:\n",
        "            for j in range(self.n_classes_):\n",
        "                class_scores[:, j] += self.learning_rate * tree_list[j].predict(X)\n",
        "\n",
        "        # Convert to probabilities using softmax\n",
        "        exp_scores = np.exp(class_scores - np.max(class_scores, axis=1, keepdims=True))\n",
        "        probabilities = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "        return probabilities\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class for given data.\n",
        "        \"\"\"\n",
        "        probabilities = self.predict_proba(X)\n",
        "        return np.argmax(probabilities, axis=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "sAc861uYx9i7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PCA:\n",
        "    def __init__(self, n_components):\n",
        "        self.n_components = n_components\n",
        "        self.mean_ = None\n",
        "        self.components_ = None\n",
        "\n",
        "    def fit(self, X):\n",
        "        self.mean_ = np.mean(X, axis=0)\n",
        "\n",
        "        X_centered = X - self.mean_\n",
        "\n",
        "        _, _, V = np.linalg.svd(X_centered, full_matrices=False)\n",
        "\n",
        "        self.components_ = V[:self.n_components]\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_centered = X - self.mean_\n",
        "        return np.dot(X_centered, self.components_.T)\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        self.fit(X)\n",
        "        return self.transform(X)"
      ],
      "metadata": {
        "id": "lkP7aV73zBXU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxWAE6ByTCQD",
        "outputId": "3cb049a6-e94a-4ccd-b52f-86084bf33564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (60000, 784)\n",
            "y_train shape: (60000,)\n",
            "X_test shape: (10000, 784)\n",
            "y_test shape: (10000,)\n",
            "Accuracy: 0.8868\n",
            "Confusion Matrix:\n",
            "[[ 933    0    2    2    1   21   12    2    5    2]\n",
            " [   0 1103    5    3    1    2    4    0   17    0]\n",
            " [  15    4  871   28   16    6   20   13   57    2]\n",
            " [   1    8   22  859    2   57    1   18   35    7]\n",
            " [   1    4    6    1  860    2   21   11    5   71]\n",
            " [  17    2    5   41   15  756   11    7   28   10]\n",
            " [  19    4    9    1   11   22  887    0    5    0]\n",
            " [   2   14   20    5   12    0    0  910    9   56]\n",
            " [  16    5   15   40   11   41    3   12  813   18]\n",
            " [   9   10    4   15   49    7    0   30    9  876]]\n",
            "\n",
            "Per-class performance analysis:\n",
            "Digit 0:\n",
            "  - Accuracy: 0.9520\n",
            "Digit 1:\n",
            "  - Accuracy: 0.9718\n",
            "Digit 2:\n",
            "  - Accuracy: 0.8440\n",
            "Digit 3:\n",
            "  - Accuracy: 0.8505\n",
            "Digit 4:\n",
            "  - Accuracy: 0.8758\n",
            "Digit 5:\n",
            "  - Accuracy: 0.8475\n",
            "Digit 6:\n",
            "  - Accuracy: 0.9259\n",
            "Digit 7:\n",
            "  - Accuracy: 0.8852\n",
            "Digit 8:\n",
            "  - Accuracy: 0.8347\n",
            "Digit 9:\n",
            "  - Accuracy: 0.8682\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "\n",
        "def read_idx_file(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        magic_number = int.from_bytes(f.read(4), byteorder='big')\n",
        "\n",
        "        if magic_number == 2051:\n",
        "            num_images = int.from_bytes(f.read(4), byteorder='big')\n",
        "            rows = int.from_bytes(f.read(4), byteorder='big')\n",
        "            cols = int.from_bytes(f.read(4), byteorder='big')\n",
        "\n",
        "            data = np.frombuffer(f.read(), dtype=np.uint8)\n",
        "            data = data.reshape(num_images, rows * cols)\n",
        "            return data\n",
        "\n",
        "        elif magic_number == 2049:\n",
        "            num_labels = int.from_bytes(f.read(4), byteorder='big')\n",
        "            labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
        "            return labels\n",
        "        else:\n",
        "            raise ValueError(\"Invalid magic number\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # load data\n",
        "    path = kagglehub.dataset_download(\"hojjatk/mnist-dataset\")\n",
        "\n",
        "    train_images_path = os.path.join(path, \"train-images.idx3-ubyte\")\n",
        "    train_labels_path = os.path.join(path, \"train-labels.idx1-ubyte\")\n",
        "\n",
        "    test_images_path = os.path.join(path, \"t10k-images.idx3-ubyte\")\n",
        "    test_labels_path = os.path.join(path, \"t10k-labels.idx1-ubyte\")\n",
        "\n",
        "    X_train, y_train = read_idx_file(train_images_path), read_idx_file(train_labels_path)\n",
        "    X_test, y_test = read_idx_file(test_images_path), read_idx_file(test_labels_path)\n",
        "\n",
        "    print(\"X_train shape:\", X_train.shape)\n",
        "    print(\"y_train shape:\", y_train.shape)\n",
        "    print(\"X_test shape:\", X_test.shape)\n",
        "    print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "    # Normalize\n",
        "    X_train = X_train / 255.0\n",
        "    X_test = X_test / 255.0\n",
        "\n",
        "    # Scale\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # PCA\n",
        "    pca = PCA(n_components=30)\n",
        "    X_train_pca = pca.fit_transform(X_train)\n",
        "    X_test_pca = pca.transform(X_test)\n",
        "\n",
        "    # Gradient Boosting\n",
        "    model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.5, max_depth=3)\n",
        "    model.fit(X_train_pca, y_train)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test_pca)\n",
        "\n",
        "    # Evaluate\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "    print(\"\\nPer-class performance analysis:\")\n",
        "    for digit in range(10):\n",
        "        digit_mask = y_test == digit\n",
        "        if np.sum(digit_mask) > 0:\n",
        "            digit_accuracy = accuracy_score(y_test[digit_mask], y_pred[digit_mask])\n",
        "\n",
        "            print(f\"Digit {digit}:\")\n",
        "            print(f\"  - Accuracy: {digit_accuracy:.4f}\")\n",
        "\n"
      ]
    }
  ]
}