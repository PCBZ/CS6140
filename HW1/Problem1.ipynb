{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4B6DSScmJO0N",
        "outputId": "811cc3ef-025d-4947-c6c0-36468d75ce36"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.9' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "%pip install ucimlrepo\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "import sklearn.tree\n",
        "\n",
        "#-----------------------------------------------------------------\n",
        "# 1. DECISION TREE IMPLEMENTATION\n",
        "#-----------------------------------------------------------------\n",
        "\n",
        "class DecisionTreeNode:\n",
        "    def __init__(self, is_leaf=False, feature_idx=None, threshold=None, value=None,\n",
        "                 left=None, right=None, gain=None):\n",
        "        # Node properties\n",
        "        self.is_leaf = is_leaf\n",
        "        self.feature_idx = feature_idx  # Feature index to split on\n",
        "        self.threshold = threshold      # Threshold value for split\n",
        "        self.value = value              # Value if leaf node (class label or regression value)\n",
        "        self.left = left                # Left child (samples where feature < threshold)\n",
        "        self.right = right              # Right child (samples where feature >= threshold)\n",
        "        self.gain = gain                # Information gain from this split\n",
        "\n",
        "class DecisionTreeClassifier:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, criterion='entropy'):\n",
        "        \"\"\"\n",
        "        Decision Tree Classifier implementation from scratch using entropy criterion\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        max_depth : int or None\n",
        "            Maximum depth of the tree\n",
        "        min_samples_split : int\n",
        "            Minimum number of samples required to split a node\n",
        "        criterion : str\n",
        "            Splitting criterion ('entropy' or 'gini')\n",
        "        \"\"\"\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.criterion = criterion\n",
        "        self.root = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Build the decision tree from training data\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : numpy array\n",
        "            Training features\n",
        "        y : numpy array\n",
        "            Target values\n",
        "        \"\"\"\n",
        "        self.n_features = X.shape[1]\n",
        "        self.n_classes = len(np.unique(y))\n",
        "        self.root = self._build_tree(X, y, depth=0)\n",
        "\n",
        "    def _build_tree(self, X, y, depth):\n",
        "        \"\"\"\n",
        "        function to build the tree\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Count of each class in current node\n",
        "        class_counts = np.bincount(y.astype(int), minlength=self.n_classes)\n",
        "\n",
        "        # Determine the class with highest count (mode)\n",
        "        predicted_class = np.argmax(class_counts)\n",
        "\n",
        "        # Create a leaf node if all samples in current node belong to same class\n",
        "        leaf_node = DecisionTreeNode(is_leaf=True, value=predicted_class)\n",
        "\n",
        "        # Create a leaf node if meets stopp criteria\n",
        "        if depth >= self.max_depth if self.max_depth else False:\n",
        "            return leaf_node\n",
        "\n",
        "        if n_samples < self.min_samples_split:\n",
        "            return leaf_node\n",
        "\n",
        "        if len(np.unique(y)) == 1:\n",
        "            return leaf_node\n",
        "\n",
        "        # Find the best split\n",
        "        best_feature, best_threshold, best_gain = self._best_split(X, y)\n",
        "\n",
        "        # If no good split is found, create a leaf node\n",
        "        if best_gain <= 0:\n",
        "            return DecisionTreeNode(is_leaf=True, value=predicted_class)\n",
        "\n",
        "        # Split the data based on the best feature and threshold\n",
        "        left_indices = X[:, best_feature] < best_threshold\n",
        "        right_indices = ~left_indices\n",
        "\n",
        "        # Recursively grow the left and right subtrees\n",
        "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
        "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
        "\n",
        "        # Return a decision node\n",
        "        return DecisionTreeNode(\n",
        "            feature_idx=best_feature,\n",
        "            threshold=best_threshold,\n",
        "            left=left_subtree,\n",
        "            right=right_subtree,\n",
        "            gain=best_gain\n",
        "        )\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        \"\"\"\n",
        "        Find the best feature and threshold for splitting the data using entropy\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        best_feature : int\n",
        "            Index of the best feature to split on\n",
        "        best_threshold : float\n",
        "            Threshold value for the best split\n",
        "        best_gain : float\n",
        "            Information gain from the best split\n",
        "        \"\"\"\n",
        "        best_gain = -1\n",
        "        best_feature = None\n",
        "        best_threshold = None\n",
        "\n",
        "        # Calculate impurity before split\n",
        "        if self.criterion == 'entropy':\n",
        "            current_ = self._entropy(y)\n",
        "        elif self.criterion == 'gini':\n",
        "            current_entropy = self._gini(y)\n",
        "\n",
        "        current_entropy = self._entropy(y)\n",
        "\n",
        "        # For each feature\n",
        "        for feature_idx in range(self.n_features):\n",
        "            # Get unique values in the feature\n",
        "            thresholds = np.unique(X[:, feature_idx])\n",
        "\n",
        "            # Skip if only one unique value\n",
        "            if len(thresholds) <= 1:\n",
        "                continue\n",
        "\n",
        "            # For potential thresholds, choose midpoints between consecutive values\n",
        "            thresholds = (thresholds[:-1] + thresholds[1:]) / 2\n",
        "\n",
        "            # For each threshold\n",
        "            for threshold in thresholds:\n",
        "                # Split the data\n",
        "                left_indices = X[:, feature_idx] < threshold\n",
        "                right_indices = ~left_indices\n",
        "\n",
        "                # Skip if split results in empty node\n",
        "                if np.sum(left_indices) == 0 or np.sum(right_indices) == 0:\n",
        "                    continue\n",
        "\n",
        "                # Calculate information gain\n",
        "                n_samples = len(y)\n",
        "                n_left = np.sum(left_indices)\n",
        "                n_right = np.sum(right_indices)\n",
        "\n",
        "                left_weight = n_left / n_samples\n",
        "                right_weight = n_right / n_samples\n",
        "\n",
        "                left_entropy = self._entropy(y[left_indices])\n",
        "                right_entropy = self._entropy(y[right_indices])\n",
        "\n",
        "                # Calculate information gain\n",
        "                gain = current_entropy - (left_weight * left_entropy + right_weight * right_entropy)\n",
        "\n",
        "                # Update best gain, feature, and threshold if better than current best\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_feature = feature_idx\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        return best_feature, best_threshold, best_gain\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        \"\"\"\n",
        "        Calculate entropy of a set of labels\n",
        "        \"\"\"\n",
        "        m = len(y)\n",
        "        if m == 0:\n",
        "            return 0\n",
        "\n",
        "        counts = np.bincount(y.astype(int), minlength=self.n_classes)\n",
        "        proportions = counts / m\n",
        "        # Avoid log(0)\n",
        "        proportions = proportions[proportions > 0]\n",
        "        return -np.sum(proportions * np.log2(proportions))\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels for samples in X\n",
        "        \"\"\"\n",
        "        return np.array([self._predict_sample(x, self.root) for x in X])\n",
        "\n",
        "    def _predict_sample(self, x, node):\n",
        "        \"\"\"\n",
        "        Predict the class label for a single sample\n",
        "        \"\"\"\n",
        "        if node.is_leaf:\n",
        "            return node.value\n",
        "\n",
        "        if x[node.feature_idx] < node.threshold:\n",
        "            return self._predict_sample(x, node.left)\n",
        "        else:\n",
        "            return self._predict_sample(x, node.right)\n",
        "\n",
        "#-----------------------------------------------------------------\n",
        "# 2. MAIN FUNCTION: SPAMBASE DATASET ANALYSIS\n",
        "#-----------------------------------------------------------------\n",
        "\n",
        "def main():\n",
        "    # Load the Spambase dataset using ucimlrepo\n",
        "    print(\"\\nFetching Spambase dataset from UCI ML Repository...\")\n",
        "\n",
        "    try:\n",
        "        # Fetch dataset using ucimlrepo\n",
        "        spambase = fetch_ucirepo(id=94)\n",
        "\n",
        "        # Extract features and targets\n",
        "        X = spambase.data.features.values\n",
        "        y = spambase.data.targets.values.ravel()\n",
        "\n",
        "        print(f\"Dataset fetched successfully: {X.shape[0]} samples, {X.shape[1]} features\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching Spambase dataset: {e}\")\n",
        "        return\n",
        "\n",
        "\n",
        "    # Check class distribution\n",
        "    unique_classes, counts = np.unique(y, return_counts=True)\n",
        "    print(\"\\nClass distribution:\")\n",
        "    for cls, count in zip(unique_classes, counts):\n",
        "        print(f\"Class {int(cls)}: {count} samples ({count / len(y) * 100:.2f}%)\")\n",
        "\n",
        "    # Set the k-fold cross-validation\n",
        "    k = 5\n",
        "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "    # Define hyperparameters\n",
        "    max_depths = [6, 8, 10]\n",
        "    min_samples_splits = [2, 20]\n",
        "\n",
        "    # Dictionary to store results\n",
        "    results = {}\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"HYPERPARAMETER TUNING WITH K-FOLD CROSS-VALIDATION (ENTROPY CRITERION)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Run k-fold cross-validation for each combination of hyperparameters\n",
        "    for max_depth in max_depths:\n",
        "        for min_samples_split in min_samples_splits:\n",
        "            key = f\"maxdepth_{max_depth}_minsplit_{min_samples_split}\"\n",
        "            results[key] = {\n",
        "                'train_acc': [],\n",
        "                'val_acc': []\n",
        "            }\n",
        "\n",
        "            print(f\"\\nEvaluating with max_depth={max_depth}, min_samples_split={min_samples_split}\")\n",
        "\n",
        "            fold = 1\n",
        "            for train_idx, val_idx in kf.split(X):\n",
        "                X_train, X_val = X[train_idx], X[val_idx]\n",
        "                y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "                # Normalize the data\n",
        "                scaler = MinMaxScaler()\n",
        "                X_train_norm, X_val_norm = scaler.fit_transform(X_train), scaler.transform(X_val)\n",
        "\n",
        "                # Train decision tree\n",
        "                clf = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split)\n",
        "                # clf = sklearn.tree.DecisionTreeClassifier(criterion='entropy', max_depth=max_depth, min_samples_split=min_samples_split)\n",
        "                clf.fit(X_train_norm, y_train)\n",
        "\n",
        "                # Evaluate on training and validation data\n",
        "                y_train_pred = clf.predict(X_train_norm)\n",
        "                y_val_pred = clf.predict(X_val_norm)\n",
        "\n",
        "                train_acc = accuracy_score(y_train, y_train_pred)\n",
        "                val_acc = accuracy_score(y_val, y_val_pred)\n",
        "\n",
        "                results[key]['train_acc'].append(train_acc)\n",
        "                results[key]['val_acc'].append(val_acc)\n",
        "\n",
        "                print(f\"  Fold {fold}: Train Acc = {train_acc:.4f}, Val Acc = {val_acc:.4f}\")\n",
        "                fold += 1\n",
        "\n",
        "            # Calculate average accuracies\n",
        "            avg_train_acc = np.mean(results[key]['train_acc'])\n",
        "            avg_val_acc = np.mean(results[key]['val_acc'])\n",
        "            std_train_acc = np.std(results[key]['train_acc'])\n",
        "            std_val_acc = np.std(results[key]['val_acc'])\n",
        "            avg_train_error = 1 - avg_train_acc\n",
        "            avg_val_error = 1 - avg_val_acc\n",
        "\n",
        "            print(f\"  Average: Train Acc = {avg_train_acc:.4f} ± {std_train_acc:.4f}, Val Acc = {avg_val_acc:.4f} ± {std_val_acc:.4f}\")\n",
        "            print(f\"  Train Error = {avg_train_error:.4f}, Val Error = {avg_val_error:.4f}\")\n",
        "\n",
        "    # Find the best hyperparameters based on validation accuracy\n",
        "    best_key = max(results.keys(), key=lambda k: np.mean(results[k]['val_acc']))\n",
        "    parts = best_key.split('_')\n",
        "    best_max_depth = parts[1]\n",
        "    best_max_depth = None if best_max_depth == 'None' else int(best_max_depth)\n",
        "    best_min_samples_split = int(parts[3])\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"BEST HYPERPARAMETERS FOUND\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Best Max Depth: {best_max_depth}\")\n",
        "    print(f\"Best Min Samples Split: {best_min_samples_split}\")\n",
        "    print(f\"Average Training Accuracy: {np.mean(results[best_key]['train_acc']):.4f}\")\n",
        "    print(f\"Average Validation Accuracy: {np.mean(results[best_key]['val_acc']):.4f}\")\n",
        "\n",
        "    # Create summary dataframe\n",
        "    summary = []\n",
        "    for max_depth in max_depths:\n",
        "        for min_samples_split in min_samples_splits:\n",
        "            key = f\"maxdepth_{max_depth}_minsplit_{min_samples_split}\"\n",
        "            summary.append({\n",
        "                'Max Depth': 'No limit' if max_depth is None else max_depth,\n",
        "                'Min Samples Split': min_samples_split,\n",
        "                'Avg Train Acc': np.mean(results[key]['train_acc']),\n",
        "                'Avg Val Acc': np.mean(results[key]['val_acc']),\n",
        "                'Avg Train Error': 1 - np.mean(results[key]['train_acc']),\n",
        "                'Avg Val Error': 1 - np.mean(results[key]['val_acc']),\n",
        "                'Train-Val Gap': np.mean(results[key]['train_acc']) - np.mean(results[key]['val_acc'])\n",
        "            })\n",
        "\n",
        "    summary_df = pd.DataFrame(summary)\n",
        "    summary_df = summary_df.sort_values('Avg Val Acc', ascending=False)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"RESULTS SUMMARY (TOP 5 SORTED BY VALIDATION ACCURACY)\")\n",
        "    print(\"=\" * 70)\n",
        "    print(summary_df.head(5).to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
        "\n",
        "    # Train the model with the best hyperparameters on the full dataset\n",
        "    # We'll split the data 80% train, 20% test for final evaluation\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"FINAL MODEL EVALUATION\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Training set size: {X_train.shape[0]} samples\")\n",
        "    print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
        "\n",
        "    # Normalize the data\n",
        "    X_train_norm, X_test_norm = scaler.fit_transform(X_train), scaler.transform(X_test)\n",
        "\n",
        "    # Train the best model\n",
        "    best_model = DecisionTreeClassifier(max_depth=best_max_depth, min_samples_split=best_min_samples_split)\n",
        "    best_model.fit(X_train_norm, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_train_pred = best_model.predict(X_train_norm)\n",
        "    y_test_pred = best_model.predict(X_test_norm)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "    print(f\"\\nBest Model Performance:\")\n",
        "    print(f\"Max Depth: {best_max_depth}\")\n",
        "    print(f\"Min Samples Split: {best_min_samples_split}\")\n",
        "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"Generalization Gap (Train-Test): {train_accuracy - test_accuracy:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6splUMXBEYg"
      },
      "source": [
        "# Spambase Dataset Decision Tree Report\n",
        "\n",
        "| Trial | Criterion | max_depth | min_samples_split | Training Accuracy | Testing Accuracy | Training Error | Testing Error |\n",
        "|-------|-----------|-----------|-------------------|-------------------|------------------|----------------|---------------|\n",
        "| 1 | Entropy | 6 | 2 | 92.89% | 91.37% | 7.11% | 8.63% |\n",
        "| 2 | Entropy | 6 | 20 | 92.68% | 91.33% | 7.32% | 8.67% |\n",
        "| 3 | Entropy | 8 | 2 | 95.09% | 92.09% | 4.91% | 7.91% |\n",
        "| 4 | Entropy | 8 | 20 | 94.53% | 92.07% | 5.47% | 7.93% |\n",
        "| 5 | Entropy | 10 | 2 | 96.60% | 92.48% | 3.40% | 7.52% |\n",
        "| 6 | Entropy | 10 | 20 | 95.46% | 92.46% | 4.54% | 7.54% |\n",
        "\n",
        "One optimal decision tree's hyperparameters could be max tree depth = 10 and max leaves = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CyTQf-YRInh",
        "outputId": "9bdf9f2b-6aff-4f4f-9824-66983f2038f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DECISION TREE REGRESSION ON HOUSING DATASET\n",
            "======================================================================\n",
            "\n",
            "Loading Housing dataset from predefined train/test files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-1-5043b07a1561>:216: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  train_data = pd.read_csv(StringIO(train_response.text), delim_whitespace=True, header=None)\n",
            "<ipython-input-1-5043b07a1561>:222: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  test_data = pd.read_csv(StringIO(test_response.text), delim_whitespace=True, header=None)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded successfully:\n",
            "  Training set: 433 samples, 13 features\n",
            "  Testing set: 74 samples, 13 features\n",
            "\n",
            "======================================================================\n",
            "HYPERPARAMETER TUNING FOR REGRESSION TREE\n",
            "======================================================================\n",
            "\n",
            "Evaluating with max_depth=3, min_samples_split=2\n",
            "  Train MSE: 15.7321\n",
            "  Test MSE: 52.2825\n",
            "\n",
            "Evaluating with max_depth=3, min_samples_split=5\n",
            "  Train MSE: 15.7321\n",
            "  Test MSE: 52.2825\n",
            "\n",
            "Evaluating with max_depth=3, min_samples_split=10\n",
            "  Train MSE: 15.7321\n",
            "  Test MSE: 52.2825\n",
            "\n",
            "Evaluating with max_depth=3, min_samples_split=20\n",
            "  Train MSE: 15.7321\n",
            "  Test MSE: 52.2825\n",
            "\n",
            "Evaluating with max_depth=5, min_samples_split=2\n",
            "  Train MSE: 6.6557\n",
            "  Test MSE: 32.3248\n",
            "\n",
            "Evaluating with max_depth=5, min_samples_split=5\n",
            "  Train MSE: 6.9846\n",
            "  Test MSE: 34.2883\n",
            "\n",
            "Evaluating with max_depth=5, min_samples_split=10\n",
            "  Train MSE: 7.8870\n",
            "  Test MSE: 44.6765\n",
            "\n",
            "Evaluating with max_depth=5, min_samples_split=20\n",
            "  Train MSE: 7.8870\n",
            "  Test MSE: 44.6765\n",
            "\n",
            "Evaluating with max_depth=8, min_samples_split=2\n",
            "  Train MSE: 2.2389\n",
            "  Test MSE: 34.3243\n",
            "\n",
            "Evaluating with max_depth=8, min_samples_split=5\n",
            "  Train MSE: 2.8374\n",
            "  Test MSE: 36.3687\n",
            "\n",
            "Evaluating with max_depth=8, min_samples_split=10\n",
            "  Train MSE: 4.5629\n",
            "  Test MSE: 45.4727\n",
            "\n",
            "Evaluating with max_depth=8, min_samples_split=20\n",
            "  Train MSE: 6.5134\n",
            "  Test MSE: 44.6668\n",
            "\n",
            "======================================================================\n",
            "BEST HYPERPARAMETERS FOUND\n",
            "======================================================================\n",
            "Best Max Depth: 5\n",
            "Best Min Samples Split: 2\n",
            "Training MSE: 6.6557\n",
            "Testing MSE: 32.3248\n",
            "\n",
            "======================================================================\n",
            "FINAL MODEL EVALUATION\n",
            "======================================================================\n",
            "\n",
            "Best Model Performance:\n",
            "Max Depth: 5\n",
            "Min Samples Split: 2\n",
            "Training MSE: 6.6557\n",
            "Testing MSE: 32.3248\n",
            "\n",
            "This concludes the implementation and analysis of regression trees for the Housing dataset.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import requests\n",
        "from io import StringIO\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------\n",
        "# 1. DECISION TREE IMPLEMENTATION\n",
        "#-----------------------------------------------------------------\n",
        "\n",
        "class DecisionTreeNode:\n",
        "    def __init__(self, is_leaf=False, feature_idx=None, threshold=None, value=None,\n",
        "                 left=None, right=None, mse_reduction=None):\n",
        "        # Node properties\n",
        "        self.is_leaf = is_leaf\n",
        "        self.feature_idx = feature_idx  # Feature index to split on\n",
        "        self.threshold = threshold      # Threshold value for split\n",
        "        self.value = value              # Value if leaf node (mean value for regression)\n",
        "        self.left = left                # Left child (samples where feature < threshold)\n",
        "        self.right = right              # Right child (samples where feature >= threshold)\n",
        "        self.mse_reduction = mse_reduction  # MSE reduction from this split\n",
        "\n",
        "class DecisionTreeRegressor:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, min_mse_reduction=0.0):\n",
        "        \"\"\"\n",
        "        Decision Tree Regressor implementation\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        max_depth : int or None\n",
        "            Maximum depth of the tree\n",
        "        min_samples_split : int\n",
        "            Minimum number of samples required to split a node\n",
        "        min_mse_reduction : float\n",
        "            Minimum reduction in MSE required for a split to be considered\n",
        "        \"\"\"\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_mse_reduction = min_mse_reduction\n",
        "        self.root = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Build the regression tree from training data\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : numpy array\n",
        "            Training features\n",
        "        y : numpy array\n",
        "            Target values\n",
        "        \"\"\"\n",
        "        self.n_features = X.shape[1]\n",
        "        self.root = self._build_tree(X, y, depth=0)\n",
        "\n",
        "    def _build_tree(self, X, y, depth):\n",
        "        \"\"\"\n",
        "        Recursive function to build the tree\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Mean of the target values in the current node\n",
        "        node_value = np.mean(y)\n",
        "\n",
        "        leaf_node = DecisionTreeNode(is_leaf=True, value=node_value)\n",
        "\n",
        "        # Create a leaf node if meets stopping criteria\n",
        "        if depth >= self.max_depth if self.max_depth else False:\n",
        "            return leaf_node\n",
        "        if n_samples < self.min_samples_split:\n",
        "            return leaf_node\n",
        "\n",
        "        # Find the best split\n",
        "        best_feature, best_threshold, best_mse_reduction = self._best_split(X, y)\n",
        "\n",
        "        # If no good split is found, create a leaf node\n",
        "        if best_mse_reduction <= self.min_mse_reduction:\n",
        "            return leaf_node\n",
        "\n",
        "        # Split the data\n",
        "        left_indices = X[:, best_feature] < best_threshold\n",
        "        right_indices = ~left_indices\n",
        "\n",
        "        # Skip if any partition is empty\n",
        "        if np.sum(left_indices) == 0 or np.sum(right_indices) == 0:\n",
        "            return leaf_node\n",
        "\n",
        "        # build left and right\n",
        "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
        "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
        "\n",
        "        return DecisionTreeNode(\n",
        "            feature_idx=best_feature,\n",
        "            threshold=best_threshold,\n",
        "            left=left_subtree,\n",
        "            right=right_subtree,\n",
        "            mse_reduction=best_mse_reduction\n",
        "        )\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        \"\"\"\n",
        "        Find the best feature and threshold for splitting the data\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        best_feature : int\n",
        "            Index of the best feature to split on\n",
        "        best_threshold : float\n",
        "            Threshold value for the best split\n",
        "        best_mse_reduction : float\n",
        "            MSE reduction from the best split\n",
        "        \"\"\"\n",
        "        best_mse_reduction = -1\n",
        "        best_feature = None\n",
        "        best_threshold = None\n",
        "\n",
        "        # Calculate MSE before split\n",
        "        node_mse = np.mean((y - np.mean(y)) ** 2)\n",
        "\n",
        "        for feature_idx in range(self.n_features):\n",
        "            # Get unique values in the feature\n",
        "            thresholds = np.unique(X[:, feature_idx])\n",
        "\n",
        "            # Skip if only one unique value\n",
        "            if len(thresholds) <= 1:\n",
        "                continue\n",
        "\n",
        "            thresholds = (thresholds[:-1] + thresholds[1:]) / 2\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                # Split the data\n",
        "                left_indices = X[:, feature_idx] < threshold\n",
        "                right_indices = ~left_indices\n",
        "\n",
        "                # Skip if split results in empty node\n",
        "                if np.sum(left_indices) == 0 or np.sum(right_indices) == 0:\n",
        "                    continue\n",
        "\n",
        "                # Calculate MSE reduction\n",
        "                y_left = y[left_indices]\n",
        "                y_right = y[right_indices]\n",
        "\n",
        "                left_mse = np.mean((y_left - np.mean(y_left)) ** 2)\n",
        "                right_mse = np.mean((y_right - np.mean(y_right)) ** 2)\n",
        "\n",
        "                # Weighted MSE of children\n",
        "                n_left = len(y_left)\n",
        "                n_right = len(y_right)\n",
        "                n_total = n_left + n_right\n",
        "\n",
        "                weighted_mse = (n_left / n_total) * left_mse + (n_right / n_total) * right_mse\n",
        "\n",
        "                # MSE reduction\n",
        "                mse_reduction = node_mse - weighted_mse\n",
        "\n",
        "                # Update best gain, feature, and threshold if better than current best\n",
        "                if mse_reduction > best_mse_reduction:\n",
        "                    best_mse_reduction = mse_reduction\n",
        "                    best_feature = feature_idx\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        return best_feature, best_threshold, best_mse_reduction\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict target values for samples in X\n",
        "        \"\"\"\n",
        "        return np.array([self._predict_sample(x, self.root) for x in X])\n",
        "\n",
        "    def _predict_sample(self, x, node):\n",
        "        \"\"\"\n",
        "        Predict the target value for a single sample\n",
        "        \"\"\"\n",
        "        if node.is_leaf:\n",
        "            return node.value\n",
        "\n",
        "        if x[node.feature_idx] < node.threshold:\n",
        "            return self._predict_sample(x, node.left)\n",
        "        else:\n",
        "            return self._predict_sample(x, node.right)\n",
        "\n",
        "#-----------------------------------------------------------------\n",
        "# 2. LOAD HOUSING DATASET FROM PREDEFINED TRAIN/TEST FILES\n",
        "#-----------------------------------------------------------------\n",
        "\n",
        "def fetch_housing_data():\n",
        "    \"\"\"\n",
        "    Fetch Housing dataset from predefined training and testing files\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    X_train : numpy array\n",
        "        Training features\n",
        "    y_train : numpy array\n",
        "        Training targets\n",
        "    X_test : numpy array\n",
        "        Testing features\n",
        "    y_test : numpy array\n",
        "        Testing targets\n",
        "    feature_names : list\n",
        "        List of feature names\n",
        "    \"\"\"\n",
        "    # URLs for training and testing data\n",
        "    train_url = \"https://www.khoury.northeastern.edu/home/vip/teach/MLcourse/data/housing_train.txt\"\n",
        "    test_url = \"https://www.khoury.northeastern.edu/home/vip/teach/MLcourse/data/housing_test.txt\"\n",
        "\n",
        "    # Feature names\n",
        "    feature_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS',\n",
        "                    'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
        "\n",
        "    # Load training data\n",
        "    train_response = requests.get(train_url)\n",
        "    train_response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "    train_data = pd.read_csv(StringIO(train_response.text), delim_whitespace=True, header=None)\n",
        "    train_data.columns = feature_names\n",
        "\n",
        "    # Load testing data\n",
        "    test_response = requests.get(test_url)\n",
        "    test_response.raise_for_status()\n",
        "    test_data = pd.read_csv(StringIO(test_response.text), delim_whitespace=True, header=None)\n",
        "    test_data.columns = feature_names\n",
        "\n",
        "    # Extract features and targets\n",
        "    X_train = train_data.iloc[:, :-1].values\n",
        "    y_train = train_data.iloc[:, -1].values\n",
        "\n",
        "    X_test = test_data.iloc[:, :-1].values\n",
        "    y_test = test_data.iloc[:, -1].values\n",
        "\n",
        "    return X_train, y_train, X_test, y_test, feature_names\n",
        "\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate the mean squared error between true and predicted values\n",
        "    \"\"\"\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "#-----------------------------------------------------------------\n",
        "# 3. MAIN FUNCTION: HOUSING DATASET ANALYSIS\n",
        "#-----------------------------------------------------------------\n",
        "\n",
        "def main():\n",
        "    print(\"DECISION TREE REGRESSION ON HOUSING DATASET\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"\\nLoading Housing dataset from predefined train/test files...\")\n",
        "\n",
        "    # Load the Housing dataset\n",
        "    X_train, y_train, X_test, y_test, feature_names = fetch_housing_data()\n",
        "\n",
        "    print(f\"Dataset loaded successfully:\")\n",
        "    print(f\"  Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
        "    print(f\"  Testing set: {X_test.shape[0]} samples, {X_test.shape[1]} features\")\n",
        "\n",
        "    # Normalize the data\n",
        "    scaler = MinMaxScaler()\n",
        "    X_train_norm = scaler.fit_transform(X_train)\n",
        "    X_test_norm = scaler.transform(X_test)\n",
        "\n",
        "    # Define hyperparameters to tune\n",
        "    max_depths = [3, 5, 8]  # None means unlimited depth\n",
        "    min_samples_splits = [2, 5, 10, 20]\n",
        "\n",
        "    # Dictionary to store results\n",
        "    results = {}\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"HYPERPARAMETER TUNING FOR REGRESSION TREE\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Evaluate each hyperparameter combination\n",
        "    for max_depth in max_depths:\n",
        "        for min_samples_split in min_samples_splits:\n",
        "            key = f\"maxdepth_{max_depth}_minsplit_{min_samples_split}\"\n",
        "\n",
        "            print(f\"\\nEvaluating with max_depth={max_depth}, min_samples_split={min_samples_split}\")\n",
        "\n",
        "            # Train regression tree\n",
        "            regressor = DecisionTreeRegressor(max_depth=max_depth, min_samples_split=min_samples_split)\n",
        "            regressor.fit(X_train_norm, y_train)\n",
        "\n",
        "            # Predict\n",
        "            y_train_pred = regressor.predict(X_train_norm)\n",
        "            y_test_pred = regressor.predict(X_test_norm)\n",
        "\n",
        "            # Calculate metrics\n",
        "            train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "            test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "\n",
        "            # Set results\n",
        "            results[key] = {\n",
        "                'max_depth': max_depth,\n",
        "                'min_samples_split': min_samples_split,\n",
        "                'train_mse': train_mse,\n",
        "                'test_mse': test_mse,\n",
        "            }\n",
        "\n",
        "            print(f\"  Train MSE: {train_mse:.4f}\")\n",
        "            print(f\"  Test MSE: {test_mse:.4f}\")\n",
        "\n",
        "    # Find the best hyperparameters based on test MSE\n",
        "    best_key = min(results.keys(), key=lambda k: results[k]['test_mse'])\n",
        "    best_max_depth = results[best_key]['max_depth']\n",
        "    best_min_samples_split = results[best_key]['min_samples_split']\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"BEST HYPERPARAMETERS FOUND\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Best Max Depth: {best_max_depth}\")\n",
        "    print(f\"Best Min Samples Split: {best_min_samples_split}\")\n",
        "    print(f\"Training MSE: {results[best_key]['train_mse']:.4f}\")\n",
        "    print(f\"Testing MSE: {results[best_key]['test_mse']:.4f}\")\n",
        "\n",
        "    # Train the final model with the best hyperparameters\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"FINAL MODEL EVALUATION\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Train the best model\n",
        "    best_model = DecisionTreeRegressor(max_depth=best_max_depth, min_samples_split=best_min_samples_split)\n",
        "    best_model.fit(X_train_norm, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_train_pred = best_model.predict(X_train_norm)\n",
        "    y_test_pred = best_model.predict(X_test_norm)\n",
        "\n",
        "    # Calculate metrics\n",
        "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "\n",
        "    print(f\"\\nBest Model Performance:\")\n",
        "    print(f\"Max Depth: {best_max_depth}\")\n",
        "    print(f\"Min Samples Split: {best_min_samples_split}\")\n",
        "    print(f\"Training MSE: {train_mse:.4f}\")\n",
        "    print(f\"Testing MSE: {test_mse:.4f}\")\n",
        "\n",
        "    print(\"\\nThis concludes the implementation and analysis of regression trees for the Housing dataset.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi8__cHXmSjA"
      },
      "source": [
        "# Hosing Dataset Regression Tree Report\n",
        "\n",
        "| Trial | max_depth | min_samples_split | Train MSE | Test MSE |\n",
        "|-------|-----------|-------------------|-----------|----------|\n",
        "| 1 | 3 | 2 | 15.7321 | 52.2825 |\n",
        "| 2 | 3 | 5 | 15.7321 | 52.2825 |\n",
        "| 3 | 3 | 10 | 15.7321 | 52.2825 |\n",
        "| 4 | 3 | 20 | 15.7321 | 52.2825 |\n",
        "| 5 | 5 | 2 | 6.6557 | 32.3248 |\n",
        "| 6 | 5 | 5 | 6.9846 | 34.2883 |\n",
        "| 7 | 5 | 10 | 7.8870 | 44.6765 |\n",
        "| 8 | 5 | 20 | 7.8870 | 44.6765 |\n",
        "| 9 | 8 | 2 | 2.2389 | 34.3243 |\n",
        "| 10 | 8 | 5 | 2.8374 | 36.3687 |\n",
        "| 11 | 8 | 10 | 4.5629 | 45.4727 |\n",
        "| 12 | 8 | 20 | 6.5134 | 44.6668 |\n",
        "\n",
        "One optimal regression tree is max tree depth is 5, max leaves node is 5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ_Ht15Ix7Km"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
