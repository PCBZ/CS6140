{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4B6DSScmJO0N",
        "outputId": "d61994ae-988e-46f0-84d4-a0cf17e72dd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ucimlrepo in /usr/local/lib/python3.11/dist-packages (0.0.7)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ucimlrepo) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.11/dist-packages (from ucimlrepo) (2025.4.26)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n",
            "\n",
            "Fetching Spambase dataset from UCI ML Repository...\n",
            "Dataset fetched successfully: 4601 samples, 57 features\n",
            "\n",
            "Class distribution:\n",
            "Class 0: 2788 samples (60.60%)\n",
            "Class 1: 1813 samples (39.40%)\n",
            "\n",
            "======================================================================\n",
            "HYPERPARAMETER TUNING WITH K-FOLD CROSS-VALIDATION (ENTROPY CRITERION)\n",
            "======================================================================\n",
            "\n",
            "Evaluating with max_depth=6, min_samples_split=2\n",
            "  Fold 1: Train Acc = 0.9315, Val Acc = 0.9034\n",
            "  Fold 2: Train Acc = 0.9267, Val Acc = 0.9196\n",
            "  Fold 3: Train Acc = 0.9261, Val Acc = 0.9109\n",
            "  Fold 4: Train Acc = 0.9310, Val Acc = 0.9130\n",
            "  Fold 5: Train Acc = 0.9291, Val Acc = 0.9217\n",
            "  Average: Train Acc = 0.9289 ± 0.0022, Val Acc = 0.9137 ± 0.0065\n",
            "  Train Error = 0.0711, Val Error = 0.0863\n",
            "\n",
            "Evaluating with max_depth=6, min_samples_split=20\n",
            "  Fold 1: Train Acc = 0.9293, Val Acc = 0.9066\n",
            "  Fold 2: Train Acc = 0.9253, Val Acc = 0.9217\n",
            "  Fold 3: Train Acc = 0.9237, Val Acc = 0.9087\n",
            "  Fold 4: Train Acc = 0.9277, Val Acc = 0.9087\n",
            "  Fold 5: Train Acc = 0.9280, Val Acc = 0.9207\n",
            "  Average: Train Acc = 0.9268 ± 0.0020, Val Acc = 0.9133 ± 0.0065\n",
            "  Train Error = 0.0732, Val Error = 0.0867\n",
            "\n",
            "Evaluating with max_depth=8, min_samples_split=2\n",
            "  Fold 1: Train Acc = 0.9533, Val Acc = 0.9164\n",
            "  Fold 2: Train Acc = 0.9465, Val Acc = 0.9228\n",
            "  Fold 3: Train Acc = 0.9506, Val Acc = 0.9228\n",
            "  Fold 4: Train Acc = 0.9514, Val Acc = 0.9163\n",
            "  Fold 5: Train Acc = 0.9530, Val Acc = 0.9261\n",
            "  Average: Train Acc = 0.9509 ± 0.0024, Val Acc = 0.9209 ± 0.0039\n",
            "  Train Error = 0.0491, Val Error = 0.0791\n",
            "\n",
            "Evaluating with max_depth=8, min_samples_split=20\n",
            "  Fold 1: Train Acc = 0.9467, Val Acc = 0.9175\n",
            "  Fold 2: Train Acc = 0.9427, Val Acc = 0.9283\n",
            "  Fold 3: Train Acc = 0.9440, Val Acc = 0.9174\n",
            "  Fold 4: Train Acc = 0.9459, Val Acc = 0.9152\n",
            "  Fold 5: Train Acc = 0.9470, Val Acc = 0.9250\n",
            "  Average: Train Acc = 0.9453 ± 0.0017, Val Acc = 0.9207 ± 0.0050\n",
            "  Train Error = 0.0547, Val Error = 0.0793\n",
            "\n",
            "Evaluating with max_depth=10, min_samples_split=2\n",
            "  Fold 1: Train Acc = 0.9707, Val Acc = 0.9283\n",
            "  Fold 2: Train Acc = 0.9622, Val Acc = 0.9228\n",
            "  Fold 3: Train Acc = 0.9652, Val Acc = 0.9174\n",
            "  Fold 4: Train Acc = 0.9671, Val Acc = 0.9304\n",
            "  Fold 5: Train Acc = 0.9650, Val Acc = 0.9250\n",
            "  Average: Train Acc = 0.9660 ± 0.0028, Val Acc = 0.9248 ± 0.0045\n",
            "  Train Error = 0.0340, Val Error = 0.0752\n",
            "\n",
            "Evaluating with max_depth=10, min_samples_split=20\n",
            "  Fold 1: Train Acc = 0.9543, Val Acc = 0.9262\n",
            "  Fold 2: Train Acc = 0.9549, Val Acc = 0.9304\n",
            "  Fold 3: Train Acc = 0.9552, Val Acc = 0.9120\n",
            "  Fold 4: Train Acc = 0.9568, Val Acc = 0.9272\n",
            "  Fold 5: Train Acc = 0.9516, Val Acc = 0.9272\n",
            "  Average: Train Acc = 0.9546 ± 0.0017, Val Acc = 0.9246 ± 0.0065\n",
            "  Train Error = 0.0454, Val Error = 0.0754\n",
            "\n",
            "======================================================================\n",
            "RESULTS SUMMARY (TOP 5 SORTED BY VALIDATION ACCURACY)\n",
            "======================================================================\n",
            " Max Depth  Min Samples Split  Avg Train Acc  Avg Val Acc  Avg Train Error  Avg Val Error\n",
            "        10                  2         0.9660       0.9248           0.0340         0.0752\n",
            "        10                 20         0.9546       0.9246           0.0454         0.0754\n",
            "         8                  2         0.9509       0.9209           0.0491         0.0791\n",
            "         8                 20         0.9453       0.9207           0.0547         0.0793\n",
            "         6                  2         0.9289       0.9137           0.0711         0.0863\n"
          ]
        }
      ],
      "source": [
        "%pip install ucimlrepo\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "from scipy.stats import entropy\n",
        "\n",
        "#-----------------------------------------------------------------\n",
        "# 1. DECISION TREE IMPLEMENTATION\n",
        "#-----------------------------------------------------------------\n",
        "\n",
        "class DecisionTreeNode:\n",
        "    \"\"\"\n",
        "    Decision Tree Node class\n",
        "    \"\"\"\n",
        "    def __init__(self, is_leaf=False, feature_idx=None, threshold=None, value=None,\n",
        "                 left=None, right=None, gain=None):\n",
        "        self.is_leaf = is_leaf\n",
        "        self.feature_idx = feature_idx  # Feature index to split on\n",
        "        self.threshold = threshold      # Threshold value for split\n",
        "        self.value = value              # Value if leaf node (class label or regression value)\n",
        "        self.left = left                # Left child (samples where feature < threshold)\n",
        "        self.right = right              # Right child (samples where feature >= threshold)\n",
        "        self.gain = gain                # Information gain from this split\n",
        "\n",
        "class DecisionTreeClassifier:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2):\n",
        "        \"\"\"\n",
        "        Decision Tree Classifier implementation from scratch using entropy criterion\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        max_depth : int or None\n",
        "            Maximum depth of the tree\n",
        "        min_samples_split : int\n",
        "            Minimum number of samples required to split a node\n",
        "        \"\"\"\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.root = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Build the decision tree from data\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : numpy array Training features\n",
        "        y : numpy array Target values\n",
        "        \"\"\"\n",
        "        self.n_features = X.shape[1]\n",
        "        self.n_classes = len(np.unique(y))\n",
        "        self.root = self._build_tree(X, y, depth=0)\n",
        "\n",
        "    def _build_tree(self, X, y, depth):\n",
        "        \"\"\"\n",
        "        function to build the tree\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Count of each class in current node\n",
        "        class_counts = np.bincount(y.astype(int), minlength=self.n_classes)\n",
        "\n",
        "        # Determine the class with highest count (mode)\n",
        "        predicted_class = np.argmax(class_counts)\n",
        "\n",
        "        # Init a leaf node\n",
        "        leaf_node = DecisionTreeNode(is_leaf=True, value=predicted_class)\n",
        "\n",
        "        # Create a leaf node if meets stopp criteria\n",
        "        if depth >= self.max_depth if self.max_depth else False:\n",
        "            return leaf_node\n",
        "\n",
        "        if n_samples < self.min_samples_split:\n",
        "            return leaf_node\n",
        "\n",
        "        if len(np.unique(y)) == 1:\n",
        "            return leaf_node\n",
        "\n",
        "        # Find the best split\n",
        "        best_feature, best_threshold, best_gain = self._best_split(X, y)\n",
        "\n",
        "        # If no good split is found, create a leaf node\n",
        "        if best_gain <= 0:\n",
        "            return leaf_node\n",
        "\n",
        "        # Split the data based on the best feature and threshold\n",
        "        left_indices = X[:, best_feature] < best_threshold\n",
        "        right_indices = ~left_indices\n",
        "\n",
        "        # Recursively grow the left and right subtrees\n",
        "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
        "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
        "\n",
        "        # Return a decision node\n",
        "        return DecisionTreeNode(\n",
        "            feature_idx=best_feature,\n",
        "            threshold=best_threshold,\n",
        "            left=left_subtree,\n",
        "            right=right_subtree,\n",
        "            gain=best_gain\n",
        "        )\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        \"\"\"\n",
        "        Find the best feature and threshold for splitting the data using entropy\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        best_feature : int\n",
        "            Index of the best feature to split on\n",
        "        best_threshold : float\n",
        "            Threshold value for the best split\n",
        "        best_gain : float\n",
        "            Information gain from the best split\n",
        "        \"\"\"\n",
        "        best_gain = -1\n",
        "        best_feature = None\n",
        "        best_threshold = None\n",
        "\n",
        "        # For each feature\n",
        "        for feature_idx in range(self.n_features):\n",
        "            # Get unique values in the feature\n",
        "            thresholds = np.unique(X[:, feature_idx])\n",
        "\n",
        "            # Skip if only one unique value\n",
        "            if len(thresholds) <= 1:\n",
        "                continue\n",
        "\n",
        "            # For potential thresholds, choose midpoints between consecutive values\n",
        "            thresholds = (thresholds[:-1] + thresholds[1:]) / 2\n",
        "\n",
        "            # For each threshold\n",
        "            for threshold in thresholds:\n",
        "                # Split the data\n",
        "                left_indices = X[:, feature_idx] < threshold\n",
        "                right_indices = ~left_indices\n",
        "\n",
        "                # Skip if split results in empty node\n",
        "                if np.sum(left_indices) == 0 or np.sum(right_indices) == 0:\n",
        "                    continue\n",
        "\n",
        "                # Calculate information gain\n",
        "                n_samples = len(y)\n",
        "                n_left = np.sum(left_indices)\n",
        "                n_right = np.sum(right_indices)\n",
        "\n",
        "                left_weight = n_left / n_samples\n",
        "                right_weight = n_right / n_samples\n",
        "\n",
        "                left_entropy = self._entropy(y[left_indices])\n",
        "                right_entropy = self._entropy(y[right_indices])\n",
        "\n",
        "                # Calculate information gain\n",
        "                gain = self._entropy(y) - (left_weight * left_entropy + right_weight * right_entropy)\n",
        "\n",
        "                # Update best gain, feature, and threshold if better than current best\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_feature = feature_idx\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        return best_feature, best_threshold, best_gain\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        \"\"\"\n",
        "        Calculate entropy of a set of labels\n",
        "        \"\"\"\n",
        "        m = len(y)\n",
        "        if m == 0:\n",
        "            return 0\n",
        "\n",
        "        counts = np.bincount(y.astype(int), minlength=self.n_classes)\n",
        "        return entropy(counts, base=2)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels for samples in X\n",
        "        \"\"\"\n",
        "        return np.array([self._predict_sample(x, self.root) for x in X])\n",
        "\n",
        "    def _predict_sample(self, x, node):\n",
        "        \"\"\"\n",
        "        Predict the class label for a single sample\n",
        "        \"\"\"\n",
        "        if node.is_leaf:\n",
        "            return node.value\n",
        "\n",
        "        if x[node.feature_idx] < node.threshold:\n",
        "            return self._predict_sample(x, node.left)\n",
        "        return self._predict_sample(x, node.right)\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------\n",
        "# 2. FETCH SPAMBASE DATASET DATA\n",
        "#-----------------------------------------------------------------\n",
        "def fetch_spambase_data():\n",
        "    \"\"\"\n",
        "    Fetch Spambase dataset from UCI ML Repository\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    X : numpy array\n",
        "        Features\n",
        "    y : numpy array\n",
        "        Target values\n",
        "    \"\"\"\n",
        "\n",
        "    # Fetch dataset using ucimlrepo\n",
        "    spambase = fetch_ucirepo(id=94)\n",
        "\n",
        "    # Extract features and targets\n",
        "    X = spambase.data.features.values\n",
        "    y = spambase.data.targets.values.ravel()\n",
        "    return X, y\n",
        "\n",
        "#-----------------------------------------------------------------\n",
        "# 3. MAIN FUNCTION: SPAMBASE DATASET ANALYSIS\n",
        "#-----------------------------------------------------------------\n",
        "\n",
        "def main():\n",
        "    # Load the Spambase dataset using ucimlrepo\n",
        "    print(\"\\nFetching Spambase dataset from UCI ML Repository...\")\n",
        "\n",
        "    X, y = fetch_spambase_data()\n",
        "\n",
        "    print(f\"Dataset fetched successfully: {X.shape[0]} samples, {X.shape[1]} features\")\n",
        "\n",
        "    # Check class distribution\n",
        "    unique_classes, counts = np.unique(y, return_counts=True)\n",
        "    print(\"\\nClass distribution:\")\n",
        "    for cls, count in zip(unique_classes, counts):\n",
        "        print(f\"Class {int(cls)}: {count} samples ({count / len(y) * 100:.2f}%)\")\n",
        "\n",
        "    # Set the k-fold cross-validation\n",
        "    k = 5\n",
        "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "    # Define hyperparameters\n",
        "    max_depths = [6, 8, 10]\n",
        "    min_samples_splits = [2, 20]\n",
        "\n",
        "    # Dictionary to store results\n",
        "    results = {}\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"HYPERPARAMETER TUNING WITH K-FOLD CROSS-VALIDATION (ENTROPY CRITERION)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Run k-fold cross-validation for each combination of hyperparameters\n",
        "    for max_depth in max_depths:\n",
        "        for min_samples_split in min_samples_splits:\n",
        "            key = f\"maxdepth_{max_depth}_minsplit_{min_samples_split}\"\n",
        "            results[key] = {\n",
        "                'train_acc': [],\n",
        "                'val_acc': []\n",
        "            }\n",
        "\n",
        "            print(f\"\\nEvaluating with max_depth={max_depth}, min_samples_split={min_samples_split}\")\n",
        "\n",
        "            fold = 1\n",
        "            for train_idx, val_idx in kf.split(X):\n",
        "                X_train, X_val = X[train_idx], X[val_idx]\n",
        "                y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "                # Normalize the data\n",
        "                scaler = MinMaxScaler()\n",
        "                X_train_norm, X_val_norm = scaler.fit_transform(X_train), scaler.transform(X_val)\n",
        "\n",
        "                # Train decision tree\n",
        "                clf = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split)\n",
        "                # clf = sklearn.tree.DecisionTreeClassifier(criterion='entropy', max_depth=max_depth, min_samples_split=min_samples_split)\n",
        "                clf.fit(X_train_norm, y_train)\n",
        "\n",
        "                # Evaluate on training and validation data\n",
        "                y_train_pred = clf.predict(X_train_norm)\n",
        "                y_val_pred = clf.predict(X_val_norm)\n",
        "\n",
        "                train_acc = accuracy_score(y_train, y_train_pred)\n",
        "                val_acc = accuracy_score(y_val, y_val_pred)\n",
        "\n",
        "                results[key]['train_acc'].append(train_acc)\n",
        "                results[key]['val_acc'].append(val_acc)\n",
        "\n",
        "                print(f\"  Fold {fold}: Train Acc = {train_acc:.4f}, Val Acc = {val_acc:.4f}\")\n",
        "                fold += 1\n",
        "\n",
        "            # Calculate average accuracies\n",
        "            avg_train_acc = np.mean(results[key]['train_acc'])\n",
        "            avg_val_acc = np.mean(results[key]['val_acc'])\n",
        "            std_train_acc = np.std(results[key]['train_acc'])\n",
        "            std_val_acc = np.std(results[key]['val_acc'])\n",
        "            avg_train_error = 1 - avg_train_acc\n",
        "            avg_val_error = 1 - avg_val_acc\n",
        "\n",
        "            print(f\"  Average: Train Acc = {avg_train_acc:.4f} ± {std_train_acc:.4f}, Val Acc = {avg_val_acc:.4f} ± {std_val_acc:.4f}\")\n",
        "            print(f\"  Train Error = {avg_train_error:.4f}, Val Error = {avg_val_error:.4f}\")\n",
        "\n",
        "\n",
        "    # Create summary dataframe\n",
        "    summary = []\n",
        "    for max_depth in max_depths:\n",
        "        for min_samples_split in min_samples_splits:\n",
        "            key = f\"maxdepth_{max_depth}_minsplit_{min_samples_split}\"\n",
        "            summary.append({\n",
        "                'Max Depth': 'No limit' if max_depth is None else max_depth,\n",
        "                'Min Samples Split': min_samples_split,\n",
        "                'Avg Train Acc': np.mean(results[key]['train_acc']),\n",
        "                'Avg Val Acc': np.mean(results[key]['val_acc']),\n",
        "                'Avg Train Error': 1 - np.mean(results[key]['train_acc']),\n",
        "                'Avg Val Error': 1 - np.mean(results[key]['val_acc'])\n",
        "            })\n",
        "\n",
        "    summary_df = pd.DataFrame(summary)\n",
        "    summary_df = summary_df.sort_values('Avg Val Acc', ascending=False)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"RESULTS SUMMARY (TOP 5 SORTED BY VALIDATION ACCURACY)\")\n",
        "    print(\"=\" * 70)\n",
        "    print(summary_df.head(5).to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6splUMXBEYg"
      },
      "source": [
        "# Spambase Dataset Decision Tree Report\n",
        "\n",
        "| Trial | Criterion | max_depth | min_samples_split | Training Accuracy | Testing Accuracy | Training Error | Testing Error |\n",
        "|-------|-----------|-----------|-------------------|-------------------|------------------|----------------|---------------|\n",
        "| 1 | Entropy | 6 | 2 | 92.89% | 91.37% | 7.11% | 8.63% |\n",
        "| 2 | Entropy | 6 | 20 | 92.68% | 91.33% | 7.32% | 8.67% |\n",
        "| 3 | Entropy | 8 | 2 | 95.09% | 92.09% | 4.91% | 7.91% |\n",
        "| 4 | Entropy | 8 | 20 | 94.53% | 92.07% | 5.47% | 7.93% |\n",
        "| 5 | Entropy | 10 | 2 | 96.60% | 92.48% | 3.40% | 7.52% |\n",
        "| 6 | Entropy | 10 | 20 | 95.46% | 92.46% | 4.54% | 7.54% |\n",
        "\n",
        "One optimal decision tree's hyperparameters could be max tree depth = 10 and max leaves = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "4CyTQf-YRInh",
        "outputId": "d4296619-4d0b-401c-affd-1a5418aa29f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DECISION TREE REGRESSION ON HOUSING DATASET\n",
            "======================================================================\n",
            "\n",
            "Loading Housing dataset from predefined train/test files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-88f32d0c284b>:214: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  train_data = pd.read_csv(StringIO(train_response.text), sep=\"s\\+\", header=None)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Length mismatch: Expected axis has 1 elements, new values have 14 elements",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-88f32d0c284b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-88f32d0c284b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;31m# Load the Housing dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_housing_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset loaded successfully:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-88f32d0c284b>\u001b[0m in \u001b[0;36mfetch_housing_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0mtrain_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Raise an exception for HTTP errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"s\\+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;31m# Load testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   6311\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6312\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6313\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6314\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6315\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mproperties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \"\"\"\n\u001b[1;32m    813\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mset_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAxisInt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;31m# Caller is responsible for ensuring we have an Index object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_set_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/base.py\u001b[0m in \u001b[0;36m_validate_set_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mnew_len\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mold_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     99\u001b[0m                 \u001b[0;34mf\"Length mismatch: Expected axis has {old_len} elements, new \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;34mf\"values have {new_len} elements\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected axis has 1 elements, new values have 14 elements"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import requests\n",
        "from io import StringIO\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------\n",
        "# 1. DECISION TREE IMPLEMENTATION\n",
        "#-----------------------------------------------------------------\n",
        "\n",
        "class DecisionTreeNode:\n",
        "    def __init__(self, is_leaf=False, feature_idx=None, threshold=None, value=None,\n",
        "                 left=None, right=None, mse_reduction=None):\n",
        "        # Node properties\n",
        "        self.is_leaf = is_leaf\n",
        "        self.feature_idx = feature_idx  # Feature index to split on\n",
        "        self.threshold = threshold      # Threshold value for split\n",
        "        self.value = value              # Value if leaf node (mean value for regression)\n",
        "        self.left = left                # Left child (samples where feature < threshold)\n",
        "        self.right = right              # Right child (samples where feature >= threshold)\n",
        "        self.mse_reduction = mse_reduction  # MSE reduction from this split\n",
        "\n",
        "class DecisionTreeRegressor:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, min_mse_reduction=0.0):\n",
        "        \"\"\"\n",
        "        Decision Tree Regressor implementation\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        max_depth : int or None\n",
        "            Maximum depth of the tree\n",
        "        min_samples_split : int\n",
        "            Minimum number of samples required to split a node\n",
        "        min_mse_reduction : float\n",
        "            Minimum reduction in MSE required for a split to be considered\n",
        "        \"\"\"\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_mse_reduction = min_mse_reduction\n",
        "        self.root = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Build the regression tree from training data\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : numpy array\n",
        "            Training features\n",
        "        y : numpy array\n",
        "            Target values\n",
        "        \"\"\"\n",
        "        self.n_features = X.shape[1]\n",
        "        self.root = self._build_tree(X, y, depth=0)\n",
        "\n",
        "    def _build_tree(self, X, y, depth):\n",
        "        \"\"\"\n",
        "        Recursive function to build the tree\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Mean of the target values in the current node\n",
        "        node_value = np.mean(y)\n",
        "\n",
        "        leaf_node = DecisionTreeNode(is_leaf=True, value=node_value)\n",
        "\n",
        "        # Create a leaf node if meets stopping criteria\n",
        "        if depth >= self.max_depth if self.max_depth else False:\n",
        "            return leaf_node\n",
        "        if n_samples < self.min_samples_split:\n",
        "            return leaf_node\n",
        "\n",
        "        # Find the best split\n",
        "        best_feature, best_threshold, best_mse_reduction = self._best_split(X, y)\n",
        "\n",
        "        # If no good split is found, create a leaf node\n",
        "        if best_mse_reduction <= self.min_mse_reduction:\n",
        "            return leaf_node\n",
        "\n",
        "        # Split the data\n",
        "        left_indices = X[:, best_feature] < best_threshold\n",
        "        right_indices = ~left_indices\n",
        "\n",
        "        # Skip if any partition is empty\n",
        "        if np.sum(left_indices) == 0 or np.sum(right_indices) == 0:\n",
        "            return leaf_node\n",
        "\n",
        "        # build left and right\n",
        "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
        "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
        "\n",
        "        return DecisionTreeNode(\n",
        "            feature_idx=best_feature,\n",
        "            threshold=best_threshold,\n",
        "            left=left_subtree,\n",
        "            right=right_subtree,\n",
        "            mse_reduction=best_mse_reduction\n",
        "        )\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        \"\"\"\n",
        "        Find the best feature and threshold for splitting the data\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        best_feature : int\n",
        "            Index of the best feature to split on\n",
        "        best_threshold : float\n",
        "            Threshold value for the best split\n",
        "        best_mse_reduction : float\n",
        "            MSE reduction from the best split\n",
        "        \"\"\"\n",
        "        best_mse_reduction = -1\n",
        "        best_feature = None\n",
        "        best_threshold = None\n",
        "\n",
        "        # Calculate MSE before split\n",
        "        node_mse = np.mean((y - np.mean(y)) ** 2)\n",
        "\n",
        "        for feature_idx in range(self.n_features):\n",
        "            # Get unique values in the feature\n",
        "            thresholds = np.unique(X[:, feature_idx])\n",
        "\n",
        "            # Skip if only one unique value\n",
        "            if len(thresholds) <= 1:\n",
        "                continue\n",
        "\n",
        "            thresholds = (thresholds[:-1] + thresholds[1:]) / 2\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                # Split the data\n",
        "                left_indices = X[:, feature_idx] < threshold\n",
        "                right_indices = ~left_indices\n",
        "\n",
        "                # Skip if split results in empty node\n",
        "                if np.sum(left_indices) == 0 or np.sum(right_indices) == 0:\n",
        "                    continue\n",
        "\n",
        "                # Calculate MSE reduction\n",
        "                y_left = y[left_indices]\n",
        "                y_right = y[right_indices]\n",
        "\n",
        "                left_mse = np.mean((y_left - np.mean(y_left)) ** 2)\n",
        "                right_mse = np.mean((y_right - np.mean(y_right)) ** 2)\n",
        "\n",
        "                # Weighted MSE of children\n",
        "                n_left = len(y_left)\n",
        "                n_right = len(y_right)\n",
        "                n_total = n_left + n_right\n",
        "\n",
        "                # MSE reduction\n",
        "                mse_reduction = node_mse - ((n_left / n_total) * left_mse + (n_right / n_total) * right_mse)\n",
        "\n",
        "                # Update best gain, feature, and threshold if better than current best\n",
        "                if mse_reduction > best_mse_reduction:\n",
        "                    best_mse_reduction = mse_reduction\n",
        "                    best_feature = feature_idx\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        return best_feature, best_threshold, best_mse_reduction\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict target values for samples in X\n",
        "        \"\"\"\n",
        "        return np.array([self._predict_sample(x, self.root) for x in X])\n",
        "\n",
        "    def _predict_sample(self, x, node):\n",
        "        \"\"\"\n",
        "        Predict the target value for a single sample\n",
        "        \"\"\"\n",
        "        if node.is_leaf:\n",
        "            return node.value\n",
        "\n",
        "        if x[node.feature_idx] < node.threshold:\n",
        "            return self._predict_sample(x, node.left)\n",
        "        else:\n",
        "            return self._predict_sample(x, node.right)\n",
        "\n",
        "#-----------------------------------------------------------------\n",
        "# 2. LOAD HOUSING DATASET FROM PREDEFINED TRAIN/TEST FILES\n",
        "#-----------------------------------------------------------------\n",
        "\n",
        "def fetch_housing_data():\n",
        "    \"\"\"\n",
        "    Fetch Housing dataset from predefined training and testing files\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    X_train : numpy array\n",
        "        Training features\n",
        "    y_train : numpy array\n",
        "        Training targets\n",
        "    X_test : numpy array\n",
        "        Testing features\n",
        "    y_test : numpy array\n",
        "        Testing targets\n",
        "    feature_names : list\n",
        "        List of feature names\n",
        "    \"\"\"\n",
        "    # URLs for training and testing data\n",
        "    train_url = \"https://www.khoury.northeastern.edu/home/vip/teach/MLcourse/data/housing_train.txt\"\n",
        "    test_url = \"https://www.khoury.northeastern.edu/home/vip/teach/MLcourse/data/housing_test.txt\"\n",
        "\n",
        "    # Feature names\n",
        "    feature_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS',\n",
        "                    'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
        "\n",
        "    # Load training data\n",
        "    train_response = requests.get(train_url)\n",
        "    train_response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "    train_data = pd.read_csv(StringIO(train_response.text), sep=\"s\\+\", header=None)\n",
        "    train_data.columns = feature_names\n",
        "\n",
        "    # Load testing data\n",
        "    test_response = requests.get(test_url)\n",
        "    test_response.raise_for_status()\n",
        "    test_data = pd.read_csv(StringIO(test_response.text), sep=\"s\\+\", header=None)\n",
        "    test_data.columns = feature_names\n",
        "\n",
        "    # Extract features and targets\n",
        "    X_train = train_data.iloc[:, :-1].values\n",
        "    y_train = train_data.iloc[:, -1].values\n",
        "\n",
        "    X_test = test_data.iloc[:, :-1].values\n",
        "    y_test = test_data.iloc[:, -1].values\n",
        "\n",
        "    return X_train, y_train, X_test, y_test, feature_names\n",
        "\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate the mean squared error between true and predicted values\n",
        "    \"\"\"\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "#-----------------------------------------------------------------\n",
        "# 3. MAIN FUNCTION: HOUSING DATASET ANALYSIS\n",
        "#-----------------------------------------------------------------\n",
        "\n",
        "def main():\n",
        "    print(\"DECISION TREE REGRESSION ON HOUSING DATASET\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"\\nLoading Housing dataset from predefined train/test files...\")\n",
        "\n",
        "    # Load the Housing dataset\n",
        "    X_train, y_train, X_test, y_test, feature_names = fetch_housing_data()\n",
        "\n",
        "    print(f\"Dataset loaded successfully:\")\n",
        "    print(f\"  Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
        "    print(f\"  Testing set: {X_test.shape[0]} samples, {X_test.shape[1]} features\")\n",
        "\n",
        "    # Normalize the data\n",
        "    scaler = MinMaxScaler()\n",
        "    X_train_norm = scaler.fit_transform(X_train)\n",
        "    X_test_norm = scaler.transform(X_test)\n",
        "\n",
        "    # Define hyperparameters to tune\n",
        "    max_depths = [3, 5, 8]  # None means unlimited depth\n",
        "    min_samples_splits = [2, 5, 10, 20]\n",
        "\n",
        "    # Dictionary to store results\n",
        "    results = {}\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"HYPERPARAMETER TUNING FOR REGRESSION TREE\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Evaluate each hyperparameter combination\n",
        "    for max_depth in max_depths:\n",
        "        for min_samples_split in min_samples_splits:\n",
        "            key = f\"maxdepth_{max_depth}_minsplit_{min_samples_split}\"\n",
        "\n",
        "            print(f\"\\nEvaluating with max_depth={max_depth}, min_samples_split={min_samples_split}\")\n",
        "\n",
        "            # Train regression tree\n",
        "            regressor = DecisionTreeRegressor(max_depth=max_depth, min_samples_split=min_samples_split)\n",
        "            regressor.fit(X_train_norm, y_train)\n",
        "\n",
        "            # Predict\n",
        "            y_train_pred = regressor.predict(X_train_norm)\n",
        "            y_test_pred = regressor.predict(X_test_norm)\n",
        "\n",
        "            # Calculate metrics\n",
        "            train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "            test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "\n",
        "            # Set results\n",
        "            results[key] = {\n",
        "                'max_depth': max_depth,\n",
        "                'min_samples_split': min_samples_split,\n",
        "                'train_mse': train_mse,\n",
        "                'test_mse': test_mse,\n",
        "            }\n",
        "\n",
        "            print(f\"  Train MSE: {train_mse:.4f}\")\n",
        "            print(f\"  Test MSE: {test_mse:.4f}\")\n",
        "\n",
        "    # Find the best hyperparameters based on test MSE\n",
        "    best_key = min(results.keys(), key=lambda k: results[k]['test_mse'])\n",
        "    best_max_depth = results[best_key]['max_depth']\n",
        "    best_min_samples_split = results[best_key]['min_samples_split']\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"BEST HYPERPARAMETERS FOUND\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Best Max Depth: {best_max_depth}\")\n",
        "    print(f\"Best Min Samples Split: {best_min_samples_split}\")\n",
        "    print(f\"Training MSE: {results[best_key]['train_mse']:.4f}\")\n",
        "    print(f\"Testing MSE: {results[best_key]['test_mse']:.4f}\")\n",
        "\n",
        "    # Train the final model with the best hyperparameters\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"FINAL MODEL EVALUATION\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Train the best model\n",
        "    best_model = DecisionTreeRegressor(max_depth=best_max_depth, min_samples_split=best_min_samples_split)\n",
        "    best_model.fit(X_train_norm, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_train_pred = best_model.predict(X_train_norm)\n",
        "    y_test_pred = best_model.predict(X_test_norm)\n",
        "\n",
        "    # Calculate metrics\n",
        "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "\n",
        "    print(f\"\\nBest Model Performance:\")\n",
        "    print(f\"Max Depth: {best_max_depth}\")\n",
        "    print(f\"Min Samples Split: {best_min_samples_split}\")\n",
        "    print(f\"Training MSE: {train_mse:.4f}\")\n",
        "    print(f\"Testing MSE: {test_mse:.4f}\")\n",
        "\n",
        "    print(\"\\nThis concludes the implementation and analysis of regression trees for the Housing dataset.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi8__cHXmSjA"
      },
      "source": [
        "# Hosing Dataset Regression Tree Report\n",
        "\n",
        "| Trial | max_depth | min_samples_split | Train MSE | Test MSE |\n",
        "|-------|-----------|-------------------|-----------|----------|\n",
        "| 1 | 3 | 2 | 15.7321 | 52.2825 |\n",
        "| 2 | 3 | 5 | 15.7321 | 52.2825 |\n",
        "| 3 | 3 | 10 | 15.7321 | 52.2825 |\n",
        "| 4 | 3 | 20 | 15.7321 | 52.2825 |\n",
        "| 5 | 5 | 2 | 6.6557 | 32.3248 |\n",
        "| 6 | 5 | 5 | 6.9846 | 34.2883 |\n",
        "| 7 | 5 | 10 | 7.8870 | 44.6765 |\n",
        "| 8 | 5 | 20 | 7.8870 | 44.6765 |\n",
        "| 9 | 8 | 2 | 2.2389 | 34.3243 |\n",
        "| 10 | 8 | 5 | 2.8374 | 36.3687 |\n",
        "| 11 | 8 | 10 | 4.5629 | 45.4727 |\n",
        "| 12 | 8 | 20 | 6.5134 | 44.6668 |\n",
        "\n",
        "One optimal regression tree is max tree depth is 5, max leaves node is 5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ_Ht15Ix7Km"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
